{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"s4dd The API documentation for s4dd , an easy-to-use python library to use structured state-space sequence (S4) models for de novo drug design. See our GitHub repository for information on installation and usage.","title":"Homepage"},{"location":"#s4dd","text":"The API documentation for s4dd , an easy-to-use python library to use structured state-space sequence (S4) models for de novo drug design. See our GitHub repository for information on installation and usage.","title":"s4dd"},{"location":"api/dataloaders/","text":"dataloaders PaddedLabelEncodedDataset Bases: torch . utils . data . Dataset A dataset that returns a tuple of (X, y) where X and y are both torch tensors. X is a sequence of integers representing the SMILES tokens, and y is the same sequence shifted by one position to the right. The outputs are padded to the same length and label encoded. Source code in s4dd/dataloaders.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 class PaddedLabelEncodedDataset ( torch . utils . data . Dataset ): \"\"\"A dataset that returns a tuple of `(X, y)` where `X` and `y` are both torch tensors. `X` is a sequence of integers representing the SMILES tokens, and `y` is the same sequence shifted by one position to the right. The outputs are padded to the same length and label encoded. \"\"\" def __init__ ( self , label_encoded_molecules : List [ List [ int ]], token2label : Dict [ str , int ], ): \"\"\"Creates a `PaddedLabelEncodedDataset`. Parameters ---------- label_encoded_molecules : List[List[int]] A list of label encoded and padded molecules, where each molecule is a list of integers representing the SMILES tokens. The integers are the labels of the tokens in the token2label dictionary. All molecules must be padded to the same length. token2label : Dict[str, int] A dictionary mapping SMILES tokens to integer labels. \"\"\" self . label_encoded_molecules = label_encoded_molecules self . token2label = token2label def __len__ ( self ) -> int : \"\"\"Returns the number of molecules in the dataset. Returns ------- int Number of molecules in the dataset. \"\"\" return len ( self . label_encoded_molecules ) def __getitem__ ( self , idx ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\"Returns a tuple of `(X, y)` where `X` and `y` are both torch tensors. `X` is a sequence of integers representing the SMILES tokens, and `y` is the same sequence shifted by one position to the right. Parameters ---------- idx : int Index of the molecule to return. Returns ------- Tuple[torch.Tensor, torch.Tensor] A tuple of `(X, y)` where `X` and `y` are both torch tensors. `X` is a sequence of integers representing the SMILES tokens, and `y` is the same sequence shifted by one position to the right. \"\"\" molecule = self . label_encoded_molecules [ idx ] X = torch . tensor ( molecule [: - 1 ]) y = torch . tensor ( molecule [ 1 :]) return X , y __getitem__ ( idx ) Returns a tuple of (X, y) where X and y are both torch tensors. X is a sequence of integers representing the SMILES tokens, and y is the same sequence shifted by one position to the right. Parameters: Name Type Description Default idx int Index of the molecule to return. required Returns: Type Description Tuple [ torch . Tensor , torch . Tensor ] A tuple of (X, y) where X and y are both torch tensors. X is a sequence of integers representing the SMILES tokens, and y is the same sequence shifted by one position to the right. Source code in s4dd/dataloaders.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def __getitem__ ( self , idx ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\"Returns a tuple of `(X, y)` where `X` and `y` are both torch tensors. `X` is a sequence of integers representing the SMILES tokens, and `y` is the same sequence shifted by one position to the right. Parameters ---------- idx : int Index of the molecule to return. Returns ------- Tuple[torch.Tensor, torch.Tensor] A tuple of `(X, y)` where `X` and `y` are both torch tensors. `X` is a sequence of integers representing the SMILES tokens, and `y` is the same sequence shifted by one position to the right. \"\"\" molecule = self . label_encoded_molecules [ idx ] X = torch . tensor ( molecule [: - 1 ]) y = torch . tensor ( molecule [ 1 :]) return X , y __init__ ( label_encoded_molecules , token2label ) Creates a PaddedLabelEncodedDataset . Parameters: Name Type Description Default label_encoded_molecules List [ List [ int ]] A list of label encoded and padded molecules, where each molecule is a list of integers representing the SMILES tokens. The integers are the labels of the tokens in the token2label dictionary. All molecules must be padded to the same length. required token2label Dict [ str , int ] A dictionary mapping SMILES tokens to integer labels. required Source code in s4dd/dataloaders.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def __init__ ( self , label_encoded_molecules : List [ List [ int ]], token2label : Dict [ str , int ], ): \"\"\"Creates a `PaddedLabelEncodedDataset`. Parameters ---------- label_encoded_molecules : List[List[int]] A list of label encoded and padded molecules, where each molecule is a list of integers representing the SMILES tokens. The integers are the labels of the tokens in the token2label dictionary. All molecules must be padded to the same length. token2label : Dict[str, int] A dictionary mapping SMILES tokens to integer labels. \"\"\" self . label_encoded_molecules = label_encoded_molecules self . token2label = token2label __len__ () Returns the number of molecules in the dataset. Returns: Type Description int Number of molecules in the dataset. Source code in s4dd/dataloaders.py 38 39 40 41 42 43 44 45 46 def __len__ ( self ) -> int : \"\"\"Returns the number of molecules in the dataset. Returns ------- int Number of molecules in the dataset. \"\"\" return len ( self . label_encoded_molecules ) create_dataloader ( path_to_data , batch_size , sequence_length = 100 , num_workers = 8 , shuffle = True , token2label = None ) Creates a dataloader for a dataset of SMILES strings. The input sequences will be tokenized, pre/appended with \"[BEG] / \"[END]\" tokens, label encoded, and padded to the same length. Parameters: Name Type Description Default path_to_data str Path to the dataset. Can be a zip file or a text file. The dataset must be a list of SMILES strings, one per line. required batch_size int Batch size. required sequence_length int , optional Number of tokens in the tokenized SMILES sequences. If a SMILES sequence has more tokens than this limit, it will be pre-truncated. If a sequence has less tokens than this, it will be post-padded with the value \"[PAD]\" . Note that the output sequences will be shifted by one position to the right, and the training sequence length will be sequence_length - 1 . The default is 100. 100 num_workers int , optional Number of workers for the dataloader. The default is 8. 8 shuffle bool , optional Whether to shuffle the dataset. The default is True. True token2label Dict [ str , int ], optional A dictionary mapping SMILES tokens to integer labels. If None , the labels will be learned from the dataset, which is useful to create the train dataloader. The validation and test dataloaders should use the same token2label learned during the creation of the train dataloader. The default is None . None Returns: Type Description torch . utils . data . DataLoader A dataloader for the dataset. Source code in s4dd/dataloaders.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def create_dataloader ( path_to_data : str , batch_size : int , sequence_length : int = 100 , num_workers : int = 8 , shuffle : bool = True , token2label : Dict [ str , int ] = None , ) -> torch . utils . data . DataLoader : \"\"\"Creates a dataloader for a dataset of SMILES strings. The input sequences will be tokenized, pre/appended with `\"[BEG]`/`\"[END]\"` tokens, label encoded, and padded to the same length. Parameters ---------- path_to_data : str Path to the dataset. Can be a zip file or a text file. The dataset must be a list of SMILES strings, one per line. batch_size : int Batch size. sequence_length : int, optional Number of tokens in the tokenized SMILES sequences. If a SMILES sequence has more tokens than this limit, it will be pre-truncated. If a sequence has less tokens than this, it will be post-padded with the value `\"[PAD]\"`. Note that the output sequences will be shifted by one position to the right, and the training sequence length will be `sequence_length - 1`. The default is 100. num_workers : int, optional Number of workers for the dataloader. The default is 8. shuffle : bool, optional Whether to shuffle the dataset. The default is True. token2label : Dict[str, int], optional A dictionary mapping SMILES tokens to integer labels. If `None`, the labels will be learned from the dataset, which is useful to create the train dataloader. The validation and test dataloaders should use the same `token2label` learned during the creation of the train dataloader. The default is `None`. Returns ------- torch.utils.data.DataLoader A dataloader for the dataset. \"\"\" if path_to_data . endswith ( \".zip\" ): import zipfile with open ( path_to_data , \"rb\" ) as f : with zipfile . ZipFile ( f ) as zf : fname = zf . namelist ()[ 0 ] with zf . open ( fname ) as g : dataset = g . read () . decode ( \"utf-8\" ) . splitlines () else : with open ( path_to_data , \"r\" ) as f : dataset = f . read () . splitlines () tokenized_dataset = [ [ \"[BEG]\" ] + smiles_utils . segment_smiles ( smiles ) + [ \"[END]\" ] for smiles in dataset ] if token2label is None : token2label = smiles_utils . learn_label_encoding ( tokenized_dataset ) padded_dataset = smiles_utils . pad_sequences ( tokenized_dataset , sequence_length , padding_value = \"[PAD]\" ) dataset = [[ token2label [ token ] for token in tokens ] for tokens in padded_dataset ] return torch . utils . data . DataLoader ( PaddedLabelEncodedDataset ( dataset , token2label = token2label , ), batch_size = batch_size , shuffle = shuffle , num_workers = num_workers , )","title":"dataloaders"},{"location":"api/dataloaders/#dataloaders","text":"","title":"dataloaders"},{"location":"api/dataloaders/#s4dd.dataloaders.PaddedLabelEncodedDataset","text":"Bases: torch . utils . data . Dataset A dataset that returns a tuple of (X, y) where X and y are both torch tensors. X is a sequence of integers representing the SMILES tokens, and y is the same sequence shifted by one position to the right. The outputs are padded to the same length and label encoded. Source code in s4dd/dataloaders.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 class PaddedLabelEncodedDataset ( torch . utils . data . Dataset ): \"\"\"A dataset that returns a tuple of `(X, y)` where `X` and `y` are both torch tensors. `X` is a sequence of integers representing the SMILES tokens, and `y` is the same sequence shifted by one position to the right. The outputs are padded to the same length and label encoded. \"\"\" def __init__ ( self , label_encoded_molecules : List [ List [ int ]], token2label : Dict [ str , int ], ): \"\"\"Creates a `PaddedLabelEncodedDataset`. Parameters ---------- label_encoded_molecules : List[List[int]] A list of label encoded and padded molecules, where each molecule is a list of integers representing the SMILES tokens. The integers are the labels of the tokens in the token2label dictionary. All molecules must be padded to the same length. token2label : Dict[str, int] A dictionary mapping SMILES tokens to integer labels. \"\"\" self . label_encoded_molecules = label_encoded_molecules self . token2label = token2label def __len__ ( self ) -> int : \"\"\"Returns the number of molecules in the dataset. Returns ------- int Number of molecules in the dataset. \"\"\" return len ( self . label_encoded_molecules ) def __getitem__ ( self , idx ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\"Returns a tuple of `(X, y)` where `X` and `y` are both torch tensors. `X` is a sequence of integers representing the SMILES tokens, and `y` is the same sequence shifted by one position to the right. Parameters ---------- idx : int Index of the molecule to return. Returns ------- Tuple[torch.Tensor, torch.Tensor] A tuple of `(X, y)` where `X` and `y` are both torch tensors. `X` is a sequence of integers representing the SMILES tokens, and `y` is the same sequence shifted by one position to the right. \"\"\" molecule = self . label_encoded_molecules [ idx ] X = torch . tensor ( molecule [: - 1 ]) y = torch . tensor ( molecule [ 1 :]) return X , y","title":"PaddedLabelEncodedDataset"},{"location":"api/dataloaders/#s4dd.dataloaders.PaddedLabelEncodedDataset.__getitem__","text":"Returns a tuple of (X, y) where X and y are both torch tensors. X is a sequence of integers representing the SMILES tokens, and y is the same sequence shifted by one position to the right. Parameters: Name Type Description Default idx int Index of the molecule to return. required Returns: Type Description Tuple [ torch . Tensor , torch . Tensor ] A tuple of (X, y) where X and y are both torch tensors. X is a sequence of integers representing the SMILES tokens, and y is the same sequence shifted by one position to the right. Source code in s4dd/dataloaders.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def __getitem__ ( self , idx ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\"Returns a tuple of `(X, y)` where `X` and `y` are both torch tensors. `X` is a sequence of integers representing the SMILES tokens, and `y` is the same sequence shifted by one position to the right. Parameters ---------- idx : int Index of the molecule to return. Returns ------- Tuple[torch.Tensor, torch.Tensor] A tuple of `(X, y)` where `X` and `y` are both torch tensors. `X` is a sequence of integers representing the SMILES tokens, and `y` is the same sequence shifted by one position to the right. \"\"\" molecule = self . label_encoded_molecules [ idx ] X = torch . tensor ( molecule [: - 1 ]) y = torch . tensor ( molecule [ 1 :]) return X , y","title":"__getitem__()"},{"location":"api/dataloaders/#s4dd.dataloaders.PaddedLabelEncodedDataset.__init__","text":"Creates a PaddedLabelEncodedDataset . Parameters: Name Type Description Default label_encoded_molecules List [ List [ int ]] A list of label encoded and padded molecules, where each molecule is a list of integers representing the SMILES tokens. The integers are the labels of the tokens in the token2label dictionary. All molecules must be padded to the same length. required token2label Dict [ str , int ] A dictionary mapping SMILES tokens to integer labels. required Source code in s4dd/dataloaders.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def __init__ ( self , label_encoded_molecules : List [ List [ int ]], token2label : Dict [ str , int ], ): \"\"\"Creates a `PaddedLabelEncodedDataset`. Parameters ---------- label_encoded_molecules : List[List[int]] A list of label encoded and padded molecules, where each molecule is a list of integers representing the SMILES tokens. The integers are the labels of the tokens in the token2label dictionary. All molecules must be padded to the same length. token2label : Dict[str, int] A dictionary mapping SMILES tokens to integer labels. \"\"\" self . label_encoded_molecules = label_encoded_molecules self . token2label = token2label","title":"__init__()"},{"location":"api/dataloaders/#s4dd.dataloaders.PaddedLabelEncodedDataset.__len__","text":"Returns the number of molecules in the dataset. Returns: Type Description int Number of molecules in the dataset. Source code in s4dd/dataloaders.py 38 39 40 41 42 43 44 45 46 def __len__ ( self ) -> int : \"\"\"Returns the number of molecules in the dataset. Returns ------- int Number of molecules in the dataset. \"\"\" return len ( self . label_encoded_molecules )","title":"__len__()"},{"location":"api/dataloaders/#s4dd.dataloaders.create_dataloader","text":"Creates a dataloader for a dataset of SMILES strings. The input sequences will be tokenized, pre/appended with \"[BEG] / \"[END]\" tokens, label encoded, and padded to the same length. Parameters: Name Type Description Default path_to_data str Path to the dataset. Can be a zip file or a text file. The dataset must be a list of SMILES strings, one per line. required batch_size int Batch size. required sequence_length int , optional Number of tokens in the tokenized SMILES sequences. If a SMILES sequence has more tokens than this limit, it will be pre-truncated. If a sequence has less tokens than this, it will be post-padded with the value \"[PAD]\" . Note that the output sequences will be shifted by one position to the right, and the training sequence length will be sequence_length - 1 . The default is 100. 100 num_workers int , optional Number of workers for the dataloader. The default is 8. 8 shuffle bool , optional Whether to shuffle the dataset. The default is True. True token2label Dict [ str , int ], optional A dictionary mapping SMILES tokens to integer labels. If None , the labels will be learned from the dataset, which is useful to create the train dataloader. The validation and test dataloaders should use the same token2label learned during the creation of the train dataloader. The default is None . None Returns: Type Description torch . utils . data . DataLoader A dataloader for the dataset. Source code in s4dd/dataloaders.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def create_dataloader ( path_to_data : str , batch_size : int , sequence_length : int = 100 , num_workers : int = 8 , shuffle : bool = True , token2label : Dict [ str , int ] = None , ) -> torch . utils . data . DataLoader : \"\"\"Creates a dataloader for a dataset of SMILES strings. The input sequences will be tokenized, pre/appended with `\"[BEG]`/`\"[END]\"` tokens, label encoded, and padded to the same length. Parameters ---------- path_to_data : str Path to the dataset. Can be a zip file or a text file. The dataset must be a list of SMILES strings, one per line. batch_size : int Batch size. sequence_length : int, optional Number of tokens in the tokenized SMILES sequences. If a SMILES sequence has more tokens than this limit, it will be pre-truncated. If a sequence has less tokens than this, it will be post-padded with the value `\"[PAD]\"`. Note that the output sequences will be shifted by one position to the right, and the training sequence length will be `sequence_length - 1`. The default is 100. num_workers : int, optional Number of workers for the dataloader. The default is 8. shuffle : bool, optional Whether to shuffle the dataset. The default is True. token2label : Dict[str, int], optional A dictionary mapping SMILES tokens to integer labels. If `None`, the labels will be learned from the dataset, which is useful to create the train dataloader. The validation and test dataloaders should use the same `token2label` learned during the creation of the train dataloader. The default is `None`. Returns ------- torch.utils.data.DataLoader A dataloader for the dataset. \"\"\" if path_to_data . endswith ( \".zip\" ): import zipfile with open ( path_to_data , \"rb\" ) as f : with zipfile . ZipFile ( f ) as zf : fname = zf . namelist ()[ 0 ] with zf . open ( fname ) as g : dataset = g . read () . decode ( \"utf-8\" ) . splitlines () else : with open ( path_to_data , \"r\" ) as f : dataset = f . read () . splitlines () tokenized_dataset = [ [ \"[BEG]\" ] + smiles_utils . segment_smiles ( smiles ) + [ \"[END]\" ] for smiles in dataset ] if token2label is None : token2label = smiles_utils . learn_label_encoding ( tokenized_dataset ) padded_dataset = smiles_utils . pad_sequences ( tokenized_dataset , sequence_length , padding_value = \"[PAD]\" ) dataset = [[ token2label [ token ] for token in tokens ] for tokens in padded_dataset ] return torch . utils . data . DataLoader ( PaddedLabelEncodedDataset ( dataset , token2label = token2label , ), batch_size = batch_size , shuffle = shuffle , num_workers = num_workers , )","title":"create_dataloader()"},{"location":"api/s4_for_denovo_design/","text":"s4_for_denovo_design S4forDenovoDesign A structured state space sequence (S4) model for de novo design. Source code in s4dd/s4_for_denovo_design.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 class S4forDenovoDesign : \"\"\"A structured state space sequence (S4) model for de novo design.\"\"\" def __init__ ( self , model_dim : int = 256 , state_dim : int = 64 , n_layers : int = 4 , n_ssm : int = 1 , dropout : float = 0.25 , vocab_size : int = 37 , sequence_length : int = 99 , n_max_epochs : int = 400 , learning_rate : float = 0.001 , batch_size : int = 2048 , device : str = \"cuda\" , ) -> None : \"\"\"Creates an `S4forDenovoDesign` instance. The default configurations are the ones used in the [paper](https://chemrxiv.org/engage/chemrxiv/article-details/65168004ade1178b24567cd3). Parameters ---------- model_dim : int The number of dimensions used across the model. state_dim : int The dimension of the state in the recurrent mode. n_layers : int The number of S4 layers in the model. n_ssm : int The number of state space models in each layer. dropout : float The dropout rate. vocab_size : int The size of the vocabulary. sequence_length : int The length of the sequences. n_max_epochs : int The maximum number of epochs to train for. learning_rate : float The learning rate. batch_size : int The batch size. device : str The device to put the model on, *e.g.,* `\"cuda\"` or `\"cpu\"`. \"\"\" self . model_dim = model_dim self . state_dim = state_dim self . n_layers = n_layers self . n_ssm = n_ssm self . dropout = dropout self . vocab_size = vocab_size self . sequence_length = sequence_length self . n_max_epochs = n_max_epochs self . learning_rate = learning_rate self . batch_size = batch_size self . device = device # These are set during training self . token2label = None self . label2token = None self . s4_model = StructuredStateSpaceSequenceModel ( model_dim = self . model_dim , state_dim = self . state_dim , n_layers = self . n_layers , n_ssm = self . n_ssm , dropout = self . dropout , learning_rate = self . learning_rate , sequence_length = self . sequence_length , vocab_size = self . vocab_size , ) @classmethod def from_file ( cls , loaddir : str ): \"\"\"Loads an `S4forDenovoDesign` instance from a directory. Parameters ---------- loaddir : str The directory to load the model from. Returns ------- S4forDenovoDesign The loaded model. \"\"\" with open ( f \" { loaddir } /init_arguments.json\" , \"r\" ) as f : properties = json . load ( f ) s4_model = StructuredStateSpaceSequenceModel ( model_dim = properties [ \"model_dim\" ], state_dim = properties [ \"state_dim\" ], n_layers = properties [ \"n_layers\" ], n_ssm = properties [ \"n_ssm\" ], dropout = properties [ \"dropout\" ], learning_rate = properties [ \"learning_rate\" ], sequence_length = properties [ \"sequence_length\" ], vocab_size = properties [ \"vocab_size\" ], ) s4_model . load_state_dict ( torch . load ( f \" { loaddir } /model.pt\" )) token2label = properties . pop ( \"token2label\" ) label2token = properties . pop ( \"label2token\" ) instance = cls ( ** properties ) instance . s4_model = s4_model instance . s4_model . to ( instance . device ) instance . token2label = token2label instance . label2token = { int ( label ): token for label , token in label2token . items () } return instance def _compute_loss ( self , loss_fn , X , y ): X = X . unsqueeze ( 2 ) . to ( self . device ) y = y . to ( self . device ) logits = self . s4_model ( X ) . permute ( 0 , 2 , 1 ) return loss_fn ( logits , y , ) def train ( self , training_molecules_path : str , val_molecules_path : str , callbacks : List [ torch_callbacks . TorchCallback ] = None , ) -> Dict [ str , List [ float ]]: \"\"\"Trains the model. The inputs are the paths to the training and validation molecules. The paths should point either to a .txt file that contains one SMILES per line, or to a zip file with the same structure. The optional callbacks can be used to monitor or configure training. The training history is returned as a dictionary. Parameters ---------- training_molecules_path : str The path to the training molecules. Can be a zip file or a text file. Must contain one SMILES string per line. val_molecules_path : str The path to the validation molecules. Must have the same structure as `training_molecules_path`. callbacks : List[torch_callbacks.TorchCallback], optional A list of callbacks to use during training. See the documentation of the `torch_callbacks` module for available options. Returns ------- Dict[str, List[float]] A dictionary containing the training history. The keys are `train_loss` and `val_loss` and the values are lists of the metric values at each epoch. \"\"\" self . s4_model = self . s4_model . to ( self . device ) train_dataloader = create_dataloader ( training_molecules_path , batch_size = self . batch_size , sequence_length = self . sequence_length + 1 , num_workers = 1 , shuffle = True , token2label = self . token2label , ) self . token2label = train_dataloader . dataset . token2label self . label2token = { v : k for k , v in self . token2label . items ()} val_dataloader = create_dataloader ( val_molecules_path , batch_size = self . batch_size , sequence_length = self . sequence_length + 1 , num_workers = 1 , shuffle = True , token2label = self . token2label , ) loss_fn = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( self . s4_model . parameters (), lr = self . learning_rate ) history = { \"train_loss\" : list (), \"val_loss\" : list ()} epoch_train_loss = 0 for epoch_ix in range ( self . n_max_epochs ): self . s4_model . recurrent_state = None # Training self . s4_model . train () n_train_batches = len ( train_dataloader ) epoch_train_loss = 0 for X_train , y_train in tqdm . tqdm ( train_dataloader ): optimizer . zero_grad () batch_train_loss = self . _compute_loss ( loss_fn , X_train , y_train ) epoch_train_loss += batch_train_loss . item () batch_train_loss . backward () optimizer . step () epoch_train_loss = epoch_train_loss / n_train_batches history [ \"train_loss\" ] . append ( epoch_train_loss ) # Validation self . s4_model . eval () n_val_batches = len ( val_dataloader ) epoch_val_loss = 0 for X_val , y_val in val_dataloader : batch_val_loss = self . _compute_loss ( loss_fn , X_val , y_val ) epoch_val_loss += batch_val_loss . item () epoch_val_loss = epoch_val_loss / n_val_batches history [ \"val_loss\" ] . append ( epoch_val_loss ) # Callbacks print ( f \"Epoch: { epoch_ix } \\t Loss: { epoch_train_loss } , Val Loss: { epoch_val_loss } \" ) if callbacks is not None : for callback in callbacks : callback . on_epoch_end ( epoch_ix = epoch_ix , history = history ) stop_training_flags = [ callback . stop_training for callback in callbacks ] stop_training = sum ( stop_training_flags ) > 0 if stop_training : print ( \"Training stopped early. Epoch:\" , epoch_ix ) break if np . isnan ( epoch_train_loss ) or np . isnan ( epoch_val_loss ): print ( \"Training diverged. Epoch:\" , epoch_ix ) break if callbacks is not None : for callback in callbacks : callback . on_train_end ( epoch_ix = epoch_ix , history = history ) return history @torch . no_grad () def design_molecules ( self , n_designs : int , batch_size : int , temperature : float , ) -> Tuple [ List [ str ], List [ float ]]: \"\"\"Designs molecules using the trained model. The number of designs to generate is specified by `n_designs`. The designs are generated in batches of size `batch_size`. The temperature is used to control the diversity of the generated designs. The designs and their log-likelihoods are returned as a tuple. Parameters ---------- n_designs : int The number of designs to generate. batch_size : int The batch size to use during generation. temperature : float The temperature to use during generation. Returns ------- Tuple[List[str], List[float]] A tuple containing the generated SMILES strings and their log-likelihoods. \"\"\" if self . token2label is None or self . label2token is None : raise ValueError ( \"This model is untrained.\" ) self . s4_model = self . s4_model . to ( self . device ) for module in self . s4_model . modules (): if hasattr ( module , \"setup_step\" ): module . setup_step () self . s4_model . eval () n_batches = math . ceil ( n_designs / batch_size ) designs , likelihoods = list (), list () for batch_idx in range ( n_batches ): if batch_idx == n_batches - 1 : batch_size = n_designs - batch_idx * batch_size X_test = ( torch . zeros ( batch_size , 1 ) . to ( torch . int ) + self . token2label [ \"[BEG]\" ] ) X_test = X_test . to ( self . device ) self . s4_model . reset_state ( batch_size , device = self . device ) X_test = X_test [:, 0 ] batch_designs , batch_likelihoods = list (), list () for __ in range ( self . sequence_length ): preds = self . s4_model . recurrent_step ( X_test ) softmax_preds = F . softmax ( preds , dim =- 1 ) . detach () . cpu () . numpy () . tolist () preds = preds . detach () . cpu () . numpy () . tolist () token_labels , token_likelihoods = list (), list () for pred_idx , pred in enumerate ( preds ): pred_temperature = np . exp ( np . array ( pred ) / temperature ) . tolist () pred_sum = sum ( pred_temperature ) pred_normed = [ p / pred_sum for p in pred_temperature ] probas = np . random . multinomial ( 1 , pred_normed ) token_label = np . argmax ( probas ) token_labels . append ( token_label ) token_likelihood = softmax_preds [ pred_idx ][ token_label ] token_likelihoods . append ( token_likelihood ) batch_designs . append ( token_labels ) batch_likelihoods . append ( token_likelihoods ) X_test = torch . tensor ( token_labels ) . to ( self . device ) designs . append ( np . array ( batch_designs ) . T ) likelihoods . append ( np . array ( batch_likelihoods ) . T ) designs = np . concatenate ( designs , axis = 0 ) . tolist () molecules = [ [ self . label2token [ label ] for label in design if self . label2token [ label ] not in [ \"[BEG]\" , \"[END]\" , \"[PAD]\" ] ] for design in designs ] molecule_lens = [ len ( molecule ) + 2 for molecule in molecules ] # +2 for [BEG] and [END] smiles = [ \"\" . join ( molecule ) for molecule in molecules ] loglikelihoods = np . log ( np . concatenate ( likelihoods , axis = 0 )) . tolist () mean_loglikelihoods = [ np . mean ( ll [: mol_len - 1 ]) for ll , mol_len in zip ( loglikelihoods , molecule_lens ) ] return smiles , mean_loglikelihoods @torch . no_grad () def compute_molecule_loglikelihoods ( self , molecules : List [ List [ str ]], batch_size : int ) -> List [ float ]: \"\"\"Computes the log-likelihoods of a list of molecules. The molecules are processed in batches of size `batch_size`. The log-likelihoods are returned as a list. Parameters ---------- molecules : List[List[str]] A list of SMILES strings. The input molecules are tokenized and padded (or truncated) internally to the sequence length used during training. batch_size : int The batch size to use during computation. Returns ------- List[float] A list of log-likelihoods. \"\"\" tokenized_molecules = [ [ \"[BEG]\" ] + smiles_utils . segment_smiles ( smiles ) + [ \"[END]\" ] for smiles in molecules ] padded_molecules = smiles_utils . pad_sequences ( tokenized_molecules , self . sequence_length + 1 , padding_value = \"[PAD]\" ) label_encoded_molecules = [ [ self . token2label [ token ] for token in tokens ] for tokens in padded_molecules ] self . s4_model = self . s4_model . to ( self . device ) for module in self . s4_model . modules (): if hasattr ( module , \"setup_step\" ): module . setup_step () self . s4_model . eval () n_batches = math . ceil ( len ( molecules ) / batch_size ) all_sequence_loglikelihoods = list () for batch_idx in range ( n_batches ): batch_start_idx = batch_idx * batch_size batch_end_idx = ( batch_idx + 1 ) * batch_size molecule_batch = label_encoded_molecules [ batch_start_idx : batch_end_idx ] self . s4_model . reset_state ( batch_size = len ( molecule_batch ), device = self . device ) batch_loglikelihoods = list () for label_idx in range ( self . sequence_length ): labels = [ molecule [ label_idx ] for molecule in molecule_batch ] X_test = torch . tensor ( labels , dtype = torch . int ) . to ( self . device ) preds = self . s4_model . recurrent_step ( X_test ) softmax_preds = F . softmax ( preds , dim =- 1 ) . detach () . cpu () . numpy () . tolist () log_preds = np . log ( softmax_preds ) next_token_labels = [ molecule [ label_idx + 1 ] for molecule in molecule_batch ] log_likelihoods = [ log_pred [ nt_label ] for nt_label , log_pred in zip ( next_token_labels , log_preds ) ] batch_loglikelihoods . append ( log_likelihoods ) batch_loglikelihoods = np . array ( batch_loglikelihoods ) . T . tolist () molecule_lengths = [ len ( molecule ) for molecule in tokenized_molecules [ batch_start_idx : batch_end_idx ] ] batch_sequence_loglikelihoods = [ np . mean ( ll [: mol_len - 1 ]) for ll , mol_len in zip ( batch_loglikelihoods , molecule_lengths ) ] all_sequence_loglikelihoods . extend ( batch_sequence_loglikelihoods ) return all_sequence_loglikelihoods def save ( self , path : str ): \"\"\"Saves the model to a directory. The directory will be created if it does not exist. Parameters ---------- path : str The directory to save the model to. \"\"\" print ( \"Saving model to\" , path ) os . makedirs ( path , exist_ok = True ) torch . save ( self . s4_model . state_dict (), f \" { path } /model.pt\" ) properties = { p : v for p , v in self . __dict__ . items () if p != \"s4_model\" } with open ( f \" { path } /init_arguments.json\" , \"w\" ) as f : json . dump ( properties , f , indent = 4 ) __init__ ( model_dim = 256 , state_dim = 64 , n_layers = 4 , n_ssm = 1 , dropout = 0.25 , vocab_size = 37 , sequence_length = 99 , n_max_epochs = 400 , learning_rate = 0.001 , batch_size = 2048 , device = 'cuda' ) Creates an S4forDenovoDesign instance. The default configurations are the ones used in the paper . Parameters: Name Type Description Default model_dim int The number of dimensions used across the model. 256 state_dim int The dimension of the state in the recurrent mode. 64 n_layers int The number of S4 layers in the model. 4 n_ssm int The number of state space models in each layer. 1 dropout float The dropout rate. 0.25 vocab_size int The size of the vocabulary. 37 sequence_length int The length of the sequences. 99 n_max_epochs int The maximum number of epochs to train for. 400 learning_rate float The learning rate. 0.001 batch_size int The batch size. 2048 device str The device to put the model on, e.g., \"cuda\" or \"cpu\" . 'cuda' Source code in s4dd/s4_for_denovo_design.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 def __init__ ( self , model_dim : int = 256 , state_dim : int = 64 , n_layers : int = 4 , n_ssm : int = 1 , dropout : float = 0.25 , vocab_size : int = 37 , sequence_length : int = 99 , n_max_epochs : int = 400 , learning_rate : float = 0.001 , batch_size : int = 2048 , device : str = \"cuda\" , ) -> None : \"\"\"Creates an `S4forDenovoDesign` instance. The default configurations are the ones used in the [paper](https://chemrxiv.org/engage/chemrxiv/article-details/65168004ade1178b24567cd3). Parameters ---------- model_dim : int The number of dimensions used across the model. state_dim : int The dimension of the state in the recurrent mode. n_layers : int The number of S4 layers in the model. n_ssm : int The number of state space models in each layer. dropout : float The dropout rate. vocab_size : int The size of the vocabulary. sequence_length : int The length of the sequences. n_max_epochs : int The maximum number of epochs to train for. learning_rate : float The learning rate. batch_size : int The batch size. device : str The device to put the model on, *e.g.,* `\"cuda\"` or `\"cpu\"`. \"\"\" self . model_dim = model_dim self . state_dim = state_dim self . n_layers = n_layers self . n_ssm = n_ssm self . dropout = dropout self . vocab_size = vocab_size self . sequence_length = sequence_length self . n_max_epochs = n_max_epochs self . learning_rate = learning_rate self . batch_size = batch_size self . device = device # These are set during training self . token2label = None self . label2token = None self . s4_model = StructuredStateSpaceSequenceModel ( model_dim = self . model_dim , state_dim = self . state_dim , n_layers = self . n_layers , n_ssm = self . n_ssm , dropout = self . dropout , learning_rate = self . learning_rate , sequence_length = self . sequence_length , vocab_size = self . vocab_size , ) compute_molecule_loglikelihoods ( molecules , batch_size ) Computes the log-likelihoods of a list of molecules. The molecules are processed in batches of size batch_size . The log-likelihoods are returned as a list. Parameters: Name Type Description Default molecules List [ List [ str ]] A list of SMILES strings. The input molecules are tokenized and padded (or truncated) internally to the sequence length used during training. required batch_size int The batch size to use during computation. required Returns: Type Description List [ float ] A list of log-likelihoods. Source code in s4dd/s4_for_denovo_design.py 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 @torch . no_grad () def compute_molecule_loglikelihoods ( self , molecules : List [ List [ str ]], batch_size : int ) -> List [ float ]: \"\"\"Computes the log-likelihoods of a list of molecules. The molecules are processed in batches of size `batch_size`. The log-likelihoods are returned as a list. Parameters ---------- molecules : List[List[str]] A list of SMILES strings. The input molecules are tokenized and padded (or truncated) internally to the sequence length used during training. batch_size : int The batch size to use during computation. Returns ------- List[float] A list of log-likelihoods. \"\"\" tokenized_molecules = [ [ \"[BEG]\" ] + smiles_utils . segment_smiles ( smiles ) + [ \"[END]\" ] for smiles in molecules ] padded_molecules = smiles_utils . pad_sequences ( tokenized_molecules , self . sequence_length + 1 , padding_value = \"[PAD]\" ) label_encoded_molecules = [ [ self . token2label [ token ] for token in tokens ] for tokens in padded_molecules ] self . s4_model = self . s4_model . to ( self . device ) for module in self . s4_model . modules (): if hasattr ( module , \"setup_step\" ): module . setup_step () self . s4_model . eval () n_batches = math . ceil ( len ( molecules ) / batch_size ) all_sequence_loglikelihoods = list () for batch_idx in range ( n_batches ): batch_start_idx = batch_idx * batch_size batch_end_idx = ( batch_idx + 1 ) * batch_size molecule_batch = label_encoded_molecules [ batch_start_idx : batch_end_idx ] self . s4_model . reset_state ( batch_size = len ( molecule_batch ), device = self . device ) batch_loglikelihoods = list () for label_idx in range ( self . sequence_length ): labels = [ molecule [ label_idx ] for molecule in molecule_batch ] X_test = torch . tensor ( labels , dtype = torch . int ) . to ( self . device ) preds = self . s4_model . recurrent_step ( X_test ) softmax_preds = F . softmax ( preds , dim =- 1 ) . detach () . cpu () . numpy () . tolist () log_preds = np . log ( softmax_preds ) next_token_labels = [ molecule [ label_idx + 1 ] for molecule in molecule_batch ] log_likelihoods = [ log_pred [ nt_label ] for nt_label , log_pred in zip ( next_token_labels , log_preds ) ] batch_loglikelihoods . append ( log_likelihoods ) batch_loglikelihoods = np . array ( batch_loglikelihoods ) . T . tolist () molecule_lengths = [ len ( molecule ) for molecule in tokenized_molecules [ batch_start_idx : batch_end_idx ] ] batch_sequence_loglikelihoods = [ np . mean ( ll [: mol_len - 1 ]) for ll , mol_len in zip ( batch_loglikelihoods , molecule_lengths ) ] all_sequence_loglikelihoods . extend ( batch_sequence_loglikelihoods ) return all_sequence_loglikelihoods design_molecules ( n_designs , batch_size , temperature ) Designs molecules using the trained model. The number of designs to generate is specified by n_designs . The designs are generated in batches of size batch_size . The temperature is used to control the diversity of the generated designs. The designs and their log-likelihoods are returned as a tuple. Parameters: Name Type Description Default n_designs int The number of designs to generate. required batch_size int The batch size to use during generation. required temperature float The temperature to use during generation. required Returns: Type Description Tuple [ List [ str ], List [ float ]] A tuple containing the generated SMILES strings and their log-likelihoods. Source code in s4dd/s4_for_denovo_design.py 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 @torch . no_grad () def design_molecules ( self , n_designs : int , batch_size : int , temperature : float , ) -> Tuple [ List [ str ], List [ float ]]: \"\"\"Designs molecules using the trained model. The number of designs to generate is specified by `n_designs`. The designs are generated in batches of size `batch_size`. The temperature is used to control the diversity of the generated designs. The designs and their log-likelihoods are returned as a tuple. Parameters ---------- n_designs : int The number of designs to generate. batch_size : int The batch size to use during generation. temperature : float The temperature to use during generation. Returns ------- Tuple[List[str], List[float]] A tuple containing the generated SMILES strings and their log-likelihoods. \"\"\" if self . token2label is None or self . label2token is None : raise ValueError ( \"This model is untrained.\" ) self . s4_model = self . s4_model . to ( self . device ) for module in self . s4_model . modules (): if hasattr ( module , \"setup_step\" ): module . setup_step () self . s4_model . eval () n_batches = math . ceil ( n_designs / batch_size ) designs , likelihoods = list (), list () for batch_idx in range ( n_batches ): if batch_idx == n_batches - 1 : batch_size = n_designs - batch_idx * batch_size X_test = ( torch . zeros ( batch_size , 1 ) . to ( torch . int ) + self . token2label [ \"[BEG]\" ] ) X_test = X_test . to ( self . device ) self . s4_model . reset_state ( batch_size , device = self . device ) X_test = X_test [:, 0 ] batch_designs , batch_likelihoods = list (), list () for __ in range ( self . sequence_length ): preds = self . s4_model . recurrent_step ( X_test ) softmax_preds = F . softmax ( preds , dim =- 1 ) . detach () . cpu () . numpy () . tolist () preds = preds . detach () . cpu () . numpy () . tolist () token_labels , token_likelihoods = list (), list () for pred_idx , pred in enumerate ( preds ): pred_temperature = np . exp ( np . array ( pred ) / temperature ) . tolist () pred_sum = sum ( pred_temperature ) pred_normed = [ p / pred_sum for p in pred_temperature ] probas = np . random . multinomial ( 1 , pred_normed ) token_label = np . argmax ( probas ) token_labels . append ( token_label ) token_likelihood = softmax_preds [ pred_idx ][ token_label ] token_likelihoods . append ( token_likelihood ) batch_designs . append ( token_labels ) batch_likelihoods . append ( token_likelihoods ) X_test = torch . tensor ( token_labels ) . to ( self . device ) designs . append ( np . array ( batch_designs ) . T ) likelihoods . append ( np . array ( batch_likelihoods ) . T ) designs = np . concatenate ( designs , axis = 0 ) . tolist () molecules = [ [ self . label2token [ label ] for label in design if self . label2token [ label ] not in [ \"[BEG]\" , \"[END]\" , \"[PAD]\" ] ] for design in designs ] molecule_lens = [ len ( molecule ) + 2 for molecule in molecules ] # +2 for [BEG] and [END] smiles = [ \"\" . join ( molecule ) for molecule in molecules ] loglikelihoods = np . log ( np . concatenate ( likelihoods , axis = 0 )) . tolist () mean_loglikelihoods = [ np . mean ( ll [: mol_len - 1 ]) for ll , mol_len in zip ( loglikelihoods , molecule_lens ) ] return smiles , mean_loglikelihoods from_file ( loaddir ) classmethod Loads an S4forDenovoDesign instance from a directory. Parameters: Name Type Description Default loaddir str The directory to load the model from. required Returns: Type Description S4forDenovoDesign The loaded model. Source code in s4dd/s4_for_denovo_design.py 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 @classmethod def from_file ( cls , loaddir : str ): \"\"\"Loads an `S4forDenovoDesign` instance from a directory. Parameters ---------- loaddir : str The directory to load the model from. Returns ------- S4forDenovoDesign The loaded model. \"\"\" with open ( f \" { loaddir } /init_arguments.json\" , \"r\" ) as f : properties = json . load ( f ) s4_model = StructuredStateSpaceSequenceModel ( model_dim = properties [ \"model_dim\" ], state_dim = properties [ \"state_dim\" ], n_layers = properties [ \"n_layers\" ], n_ssm = properties [ \"n_ssm\" ], dropout = properties [ \"dropout\" ], learning_rate = properties [ \"learning_rate\" ], sequence_length = properties [ \"sequence_length\" ], vocab_size = properties [ \"vocab_size\" ], ) s4_model . load_state_dict ( torch . load ( f \" { loaddir } /model.pt\" )) token2label = properties . pop ( \"token2label\" ) label2token = properties . pop ( \"label2token\" ) instance = cls ( ** properties ) instance . s4_model = s4_model instance . s4_model . to ( instance . device ) instance . token2label = token2label instance . label2token = { int ( label ): token for label , token in label2token . items () } return instance save ( path ) Saves the model to a directory. The directory will be created if it does not exist. Parameters: Name Type Description Default path str The directory to save the model to. required Source code in s4dd/s4_for_denovo_design.py 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 def save ( self , path : str ): \"\"\"Saves the model to a directory. The directory will be created if it does not exist. Parameters ---------- path : str The directory to save the model to. \"\"\" print ( \"Saving model to\" , path ) os . makedirs ( path , exist_ok = True ) torch . save ( self . s4_model . state_dict (), f \" { path } /model.pt\" ) properties = { p : v for p , v in self . __dict__ . items () if p != \"s4_model\" } with open ( f \" { path } /init_arguments.json\" , \"w\" ) as f : json . dump ( properties , f , indent = 4 ) train ( training_molecules_path , val_molecules_path , callbacks = None ) Trains the model. The inputs are the paths to the training and validation molecules. The paths should point either to a .txt file that contains one SMILES per line, or to a zip file with the same structure. The optional callbacks can be used to monitor or configure training. The training history is returned as a dictionary. Parameters: Name Type Description Default training_molecules_path str The path to the training molecules. Can be a zip file or a text file. Must contain one SMILES string per line. required val_molecules_path str The path to the validation molecules. Must have the same structure as training_molecules_path . required callbacks List [ torch_callbacks . TorchCallback ], optional A list of callbacks to use during training. See the documentation of the torch_callbacks module for available options. None Returns: Type Description Dict [ str , List [ float ]] A dictionary containing the training history. The keys are train_loss and val_loss and the values are lists of the metric values at each epoch. Source code in s4dd/s4_for_denovo_design.py 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 def train ( self , training_molecules_path : str , val_molecules_path : str , callbacks : List [ torch_callbacks . TorchCallback ] = None , ) -> Dict [ str , List [ float ]]: \"\"\"Trains the model. The inputs are the paths to the training and validation molecules. The paths should point either to a .txt file that contains one SMILES per line, or to a zip file with the same structure. The optional callbacks can be used to monitor or configure training. The training history is returned as a dictionary. Parameters ---------- training_molecules_path : str The path to the training molecules. Can be a zip file or a text file. Must contain one SMILES string per line. val_molecules_path : str The path to the validation molecules. Must have the same structure as `training_molecules_path`. callbacks : List[torch_callbacks.TorchCallback], optional A list of callbacks to use during training. See the documentation of the `torch_callbacks` module for available options. Returns ------- Dict[str, List[float]] A dictionary containing the training history. The keys are `train_loss` and `val_loss` and the values are lists of the metric values at each epoch. \"\"\" self . s4_model = self . s4_model . to ( self . device ) train_dataloader = create_dataloader ( training_molecules_path , batch_size = self . batch_size , sequence_length = self . sequence_length + 1 , num_workers = 1 , shuffle = True , token2label = self . token2label , ) self . token2label = train_dataloader . dataset . token2label self . label2token = { v : k for k , v in self . token2label . items ()} val_dataloader = create_dataloader ( val_molecules_path , batch_size = self . batch_size , sequence_length = self . sequence_length + 1 , num_workers = 1 , shuffle = True , token2label = self . token2label , ) loss_fn = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( self . s4_model . parameters (), lr = self . learning_rate ) history = { \"train_loss\" : list (), \"val_loss\" : list ()} epoch_train_loss = 0 for epoch_ix in range ( self . n_max_epochs ): self . s4_model . recurrent_state = None # Training self . s4_model . train () n_train_batches = len ( train_dataloader ) epoch_train_loss = 0 for X_train , y_train in tqdm . tqdm ( train_dataloader ): optimizer . zero_grad () batch_train_loss = self . _compute_loss ( loss_fn , X_train , y_train ) epoch_train_loss += batch_train_loss . item () batch_train_loss . backward () optimizer . step () epoch_train_loss = epoch_train_loss / n_train_batches history [ \"train_loss\" ] . append ( epoch_train_loss ) # Validation self . s4_model . eval () n_val_batches = len ( val_dataloader ) epoch_val_loss = 0 for X_val , y_val in val_dataloader : batch_val_loss = self . _compute_loss ( loss_fn , X_val , y_val ) epoch_val_loss += batch_val_loss . item () epoch_val_loss = epoch_val_loss / n_val_batches history [ \"val_loss\" ] . append ( epoch_val_loss ) # Callbacks print ( f \"Epoch: { epoch_ix } \\t Loss: { epoch_train_loss } , Val Loss: { epoch_val_loss } \" ) if callbacks is not None : for callback in callbacks : callback . on_epoch_end ( epoch_ix = epoch_ix , history = history ) stop_training_flags = [ callback . stop_training for callback in callbacks ] stop_training = sum ( stop_training_flags ) > 0 if stop_training : print ( \"Training stopped early. Epoch:\" , epoch_ix ) break if np . isnan ( epoch_train_loss ) or np . isnan ( epoch_val_loss ): print ( \"Training diverged. Epoch:\" , epoch_ix ) break if callbacks is not None : for callback in callbacks : callback . on_train_end ( epoch_ix = epoch_ix , history = history ) return history StructuredStateSpaceSequenceModel Bases: nn . Module A general purpose structured state space sequence (S4) model implemented as a pytorch module. Source code in s4dd/s4_for_denovo_design.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 class StructuredStateSpaceSequenceModel ( nn . Module ): \"\"\"A general purpose structured state space sequence (S4) model implemented as a pytorch module.\"\"\" def __init__ ( self , model_dim : int , state_dim : int , n_layers : int , n_ssm : int , dropout : float , learning_rate : float , sequence_length : int , vocab_size : int , ) -> None : \"\"\"Creates a `StructuredStateSpaceSequenceModel` instance. Parameters ---------- model_dim : int The dimension of the model. state_dim : int The dimension of the state in recurrent mode. n_layers : int The number of S4 layers in the model. n_ssm : int The number of state space models in each layer. dropout : float The dropout rate. learning_rate : float The learning rate. sequence_length : int The length of the sequences. vocab_size : int The size of the vocabulary. \"\"\" super () . __init__ () self . model_dim = model_dim self . state_dim = state_dim self . n_layers = n_layers self . n_ssm = n_ssm self . dropout = dropout self . learning_rate = learning_rate self . sequence_length = sequence_length self . vocab_size = vocab_size self . layer_config = [ { \"_name_\" : \"s4\" , \"d_state\" : self . state_dim , \"n_ssm\" : self . n_ssm , }, { \"_name_\" : \"s4\" , \"d_state\" : self . state_dim , \"n_ssm\" : self . n_ssm , }, { \"_name_\" : \"ff\" }, ] self . pool_config = { \"_name_\" : \"pool\" , \"stride\" : 1 , \"expand\" : None } self . embedding = nn . Embedding ( self . vocab_size , self . model_dim ) self . model = SequenceModel ( d_model = self . model_dim , n_layers = self . n_layers , transposed = True , dropout = self . dropout , layer = self . layer_config , pool = self . pool_config , ) self . output_embedding = nn . Linear ( self . model_dim , self . vocab_size ) self . recurrent_state = None def forward ( self , batch : torch . Tensor ) -> torch . Tensor : \"\"\"Computes the forward pass of the model. The forward pass consists of embedding the input tokens, passing the embeddings through the S4 model (in convolutional mode), and then passing the output of the S4 model through a linear layer to get the logits. Parameters ---------- batch : torch.Tensor A batch of sequences of integers representing the tokens. The input shape is (batch_size, sequence_length, 1). Returns ------- torch.Tensor The logits of the model. \"\"\" batch = self . embedding ( batch ) batch = batch . view ( batch . shape [ 0 ], self . sequence_length , self . model_dim ) batch , state = self . model ( batch , state = self . recurrent_state ) self . recurrent_state = state batch = self . output_embedding ( batch ) return batch def reset_state ( self , batch_size : int , device : str = None ) -> None : \"\"\"Resets the recurrent state of the model. Used in sequential mode before processing a new batch. Parameters ---------- batch_size : int The batch size. device : str The device to put the state on, *e.g.,* `\"cuda\"` or `\"cpu\"`. \"\"\" self . recurrent_state = self . model . default_state ( batch_size , device = device ) def recurrent_step ( self , x_t ): \"\"\"Computes a single step in the recurrent mode. The internal state of the model is also updated. Parameters ---------- x_t : torch.Tensor The input token. The input shape is (batch_size, 1). Returns ------- torch.Tensor The logits resulting from the stepping. \"\"\" x_t = self . embedding ( x_t ) . view ( x_t . shape [ 0 ], 1 , self . model_dim ) x_t = x_t . squeeze ( 1 ) x_t , state = self . model . step ( x_t , state = self . recurrent_state ) self . recurrent_state = state x_t = self . output_embedding ( x_t ) return x_t __init__ ( model_dim , state_dim , n_layers , n_ssm , dropout , learning_rate , sequence_length , vocab_size ) Creates a StructuredStateSpaceSequenceModel instance. Parameters: Name Type Description Default model_dim int The dimension of the model. required state_dim int The dimension of the state in recurrent mode. required n_layers int The number of S4 layers in the model. required n_ssm int The number of state space models in each layer. required dropout float The dropout rate. required learning_rate float The learning rate. required sequence_length int The length of the sequences. required vocab_size int The size of the vocabulary. required Source code in s4dd/s4_for_denovo_design.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def __init__ ( self , model_dim : int , state_dim : int , n_layers : int , n_ssm : int , dropout : float , learning_rate : float , sequence_length : int , vocab_size : int , ) -> None : \"\"\"Creates a `StructuredStateSpaceSequenceModel` instance. Parameters ---------- model_dim : int The dimension of the model. state_dim : int The dimension of the state in recurrent mode. n_layers : int The number of S4 layers in the model. n_ssm : int The number of state space models in each layer. dropout : float The dropout rate. learning_rate : float The learning rate. sequence_length : int The length of the sequences. vocab_size : int The size of the vocabulary. \"\"\" super () . __init__ () self . model_dim = model_dim self . state_dim = state_dim self . n_layers = n_layers self . n_ssm = n_ssm self . dropout = dropout self . learning_rate = learning_rate self . sequence_length = sequence_length self . vocab_size = vocab_size self . layer_config = [ { \"_name_\" : \"s4\" , \"d_state\" : self . state_dim , \"n_ssm\" : self . n_ssm , }, { \"_name_\" : \"s4\" , \"d_state\" : self . state_dim , \"n_ssm\" : self . n_ssm , }, { \"_name_\" : \"ff\" }, ] self . pool_config = { \"_name_\" : \"pool\" , \"stride\" : 1 , \"expand\" : None } self . embedding = nn . Embedding ( self . vocab_size , self . model_dim ) self . model = SequenceModel ( d_model = self . model_dim , n_layers = self . n_layers , transposed = True , dropout = self . dropout , layer = self . layer_config , pool = self . pool_config , ) self . output_embedding = nn . Linear ( self . model_dim , self . vocab_size ) self . recurrent_state = None forward ( batch ) Computes the forward pass of the model. The forward pass consists of embedding the input tokens, passing the embeddings through the S4 model (in convolutional mode), and then passing the output of the S4 model through a linear layer to get the logits. Parameters: Name Type Description Default batch torch . Tensor A batch of sequences of integers representing the tokens. The input shape is (batch_size, sequence_length, 1). required Returns: Type Description torch . Tensor The logits of the model. Source code in s4dd/s4_for_denovo_design.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def forward ( self , batch : torch . Tensor ) -> torch . Tensor : \"\"\"Computes the forward pass of the model. The forward pass consists of embedding the input tokens, passing the embeddings through the S4 model (in convolutional mode), and then passing the output of the S4 model through a linear layer to get the logits. Parameters ---------- batch : torch.Tensor A batch of sequences of integers representing the tokens. The input shape is (batch_size, sequence_length, 1). Returns ------- torch.Tensor The logits of the model. \"\"\" batch = self . embedding ( batch ) batch = batch . view ( batch . shape [ 0 ], self . sequence_length , self . model_dim ) batch , state = self . model ( batch , state = self . recurrent_state ) self . recurrent_state = state batch = self . output_embedding ( batch ) return batch recurrent_step ( x_t ) Computes a single step in the recurrent mode. The internal state of the model is also updated. Parameters: Name Type Description Default x_t torch . Tensor The input token. The input shape is (batch_size, 1). required Returns: Type Description torch . Tensor The logits resulting from the stepping. Source code in s4dd/s4_for_denovo_design.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def recurrent_step ( self , x_t ): \"\"\"Computes a single step in the recurrent mode. The internal state of the model is also updated. Parameters ---------- x_t : torch.Tensor The input token. The input shape is (batch_size, 1). Returns ------- torch.Tensor The logits resulting from the stepping. \"\"\" x_t = self . embedding ( x_t ) . view ( x_t . shape [ 0 ], 1 , self . model_dim ) x_t = x_t . squeeze ( 1 ) x_t , state = self . model . step ( x_t , state = self . recurrent_state ) self . recurrent_state = state x_t = self . output_embedding ( x_t ) return x_t reset_state ( batch_size , device = None ) Resets the recurrent state of the model. Used in sequential mode before processing a new batch. Parameters: Name Type Description Default batch_size int The batch size. required device str The device to put the state on, e.g., \"cuda\" or \"cpu\" . None Source code in s4dd/s4_for_denovo_design.py 111 112 113 114 115 116 117 118 119 120 121 122 def reset_state ( self , batch_size : int , device : str = None ) -> None : \"\"\"Resets the recurrent state of the model. Used in sequential mode before processing a new batch. Parameters ---------- batch_size : int The batch size. device : str The device to put the state on, *e.g.,* `\"cuda\"` or `\"cpu\"`. \"\"\" self . recurrent_state = self . model . default_state ( batch_size , device = device )","title":"s4_for_denovo_design"},{"location":"api/s4_for_denovo_design/#s4_for_denovo_design","text":"","title":"s4_for_denovo_design"},{"location":"api/s4_for_denovo_design/#s4dd.s4_for_denovo_design.S4forDenovoDesign","text":"A structured state space sequence (S4) model for de novo design. Source code in s4dd/s4_for_denovo_design.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 class S4forDenovoDesign : \"\"\"A structured state space sequence (S4) model for de novo design.\"\"\" def __init__ ( self , model_dim : int = 256 , state_dim : int = 64 , n_layers : int = 4 , n_ssm : int = 1 , dropout : float = 0.25 , vocab_size : int = 37 , sequence_length : int = 99 , n_max_epochs : int = 400 , learning_rate : float = 0.001 , batch_size : int = 2048 , device : str = \"cuda\" , ) -> None : \"\"\"Creates an `S4forDenovoDesign` instance. The default configurations are the ones used in the [paper](https://chemrxiv.org/engage/chemrxiv/article-details/65168004ade1178b24567cd3). Parameters ---------- model_dim : int The number of dimensions used across the model. state_dim : int The dimension of the state in the recurrent mode. n_layers : int The number of S4 layers in the model. n_ssm : int The number of state space models in each layer. dropout : float The dropout rate. vocab_size : int The size of the vocabulary. sequence_length : int The length of the sequences. n_max_epochs : int The maximum number of epochs to train for. learning_rate : float The learning rate. batch_size : int The batch size. device : str The device to put the model on, *e.g.,* `\"cuda\"` or `\"cpu\"`. \"\"\" self . model_dim = model_dim self . state_dim = state_dim self . n_layers = n_layers self . n_ssm = n_ssm self . dropout = dropout self . vocab_size = vocab_size self . sequence_length = sequence_length self . n_max_epochs = n_max_epochs self . learning_rate = learning_rate self . batch_size = batch_size self . device = device # These are set during training self . token2label = None self . label2token = None self . s4_model = StructuredStateSpaceSequenceModel ( model_dim = self . model_dim , state_dim = self . state_dim , n_layers = self . n_layers , n_ssm = self . n_ssm , dropout = self . dropout , learning_rate = self . learning_rate , sequence_length = self . sequence_length , vocab_size = self . vocab_size , ) @classmethod def from_file ( cls , loaddir : str ): \"\"\"Loads an `S4forDenovoDesign` instance from a directory. Parameters ---------- loaddir : str The directory to load the model from. Returns ------- S4forDenovoDesign The loaded model. \"\"\" with open ( f \" { loaddir } /init_arguments.json\" , \"r\" ) as f : properties = json . load ( f ) s4_model = StructuredStateSpaceSequenceModel ( model_dim = properties [ \"model_dim\" ], state_dim = properties [ \"state_dim\" ], n_layers = properties [ \"n_layers\" ], n_ssm = properties [ \"n_ssm\" ], dropout = properties [ \"dropout\" ], learning_rate = properties [ \"learning_rate\" ], sequence_length = properties [ \"sequence_length\" ], vocab_size = properties [ \"vocab_size\" ], ) s4_model . load_state_dict ( torch . load ( f \" { loaddir } /model.pt\" )) token2label = properties . pop ( \"token2label\" ) label2token = properties . pop ( \"label2token\" ) instance = cls ( ** properties ) instance . s4_model = s4_model instance . s4_model . to ( instance . device ) instance . token2label = token2label instance . label2token = { int ( label ): token for label , token in label2token . items () } return instance def _compute_loss ( self , loss_fn , X , y ): X = X . unsqueeze ( 2 ) . to ( self . device ) y = y . to ( self . device ) logits = self . s4_model ( X ) . permute ( 0 , 2 , 1 ) return loss_fn ( logits , y , ) def train ( self , training_molecules_path : str , val_molecules_path : str , callbacks : List [ torch_callbacks . TorchCallback ] = None , ) -> Dict [ str , List [ float ]]: \"\"\"Trains the model. The inputs are the paths to the training and validation molecules. The paths should point either to a .txt file that contains one SMILES per line, or to a zip file with the same structure. The optional callbacks can be used to monitor or configure training. The training history is returned as a dictionary. Parameters ---------- training_molecules_path : str The path to the training molecules. Can be a zip file or a text file. Must contain one SMILES string per line. val_molecules_path : str The path to the validation molecules. Must have the same structure as `training_molecules_path`. callbacks : List[torch_callbacks.TorchCallback], optional A list of callbacks to use during training. See the documentation of the `torch_callbacks` module for available options. Returns ------- Dict[str, List[float]] A dictionary containing the training history. The keys are `train_loss` and `val_loss` and the values are lists of the metric values at each epoch. \"\"\" self . s4_model = self . s4_model . to ( self . device ) train_dataloader = create_dataloader ( training_molecules_path , batch_size = self . batch_size , sequence_length = self . sequence_length + 1 , num_workers = 1 , shuffle = True , token2label = self . token2label , ) self . token2label = train_dataloader . dataset . token2label self . label2token = { v : k for k , v in self . token2label . items ()} val_dataloader = create_dataloader ( val_molecules_path , batch_size = self . batch_size , sequence_length = self . sequence_length + 1 , num_workers = 1 , shuffle = True , token2label = self . token2label , ) loss_fn = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( self . s4_model . parameters (), lr = self . learning_rate ) history = { \"train_loss\" : list (), \"val_loss\" : list ()} epoch_train_loss = 0 for epoch_ix in range ( self . n_max_epochs ): self . s4_model . recurrent_state = None # Training self . s4_model . train () n_train_batches = len ( train_dataloader ) epoch_train_loss = 0 for X_train , y_train in tqdm . tqdm ( train_dataloader ): optimizer . zero_grad () batch_train_loss = self . _compute_loss ( loss_fn , X_train , y_train ) epoch_train_loss += batch_train_loss . item () batch_train_loss . backward () optimizer . step () epoch_train_loss = epoch_train_loss / n_train_batches history [ \"train_loss\" ] . append ( epoch_train_loss ) # Validation self . s4_model . eval () n_val_batches = len ( val_dataloader ) epoch_val_loss = 0 for X_val , y_val in val_dataloader : batch_val_loss = self . _compute_loss ( loss_fn , X_val , y_val ) epoch_val_loss += batch_val_loss . item () epoch_val_loss = epoch_val_loss / n_val_batches history [ \"val_loss\" ] . append ( epoch_val_loss ) # Callbacks print ( f \"Epoch: { epoch_ix } \\t Loss: { epoch_train_loss } , Val Loss: { epoch_val_loss } \" ) if callbacks is not None : for callback in callbacks : callback . on_epoch_end ( epoch_ix = epoch_ix , history = history ) stop_training_flags = [ callback . stop_training for callback in callbacks ] stop_training = sum ( stop_training_flags ) > 0 if stop_training : print ( \"Training stopped early. Epoch:\" , epoch_ix ) break if np . isnan ( epoch_train_loss ) or np . isnan ( epoch_val_loss ): print ( \"Training diverged. Epoch:\" , epoch_ix ) break if callbacks is not None : for callback in callbacks : callback . on_train_end ( epoch_ix = epoch_ix , history = history ) return history @torch . no_grad () def design_molecules ( self , n_designs : int , batch_size : int , temperature : float , ) -> Tuple [ List [ str ], List [ float ]]: \"\"\"Designs molecules using the trained model. The number of designs to generate is specified by `n_designs`. The designs are generated in batches of size `batch_size`. The temperature is used to control the diversity of the generated designs. The designs and their log-likelihoods are returned as a tuple. Parameters ---------- n_designs : int The number of designs to generate. batch_size : int The batch size to use during generation. temperature : float The temperature to use during generation. Returns ------- Tuple[List[str], List[float]] A tuple containing the generated SMILES strings and their log-likelihoods. \"\"\" if self . token2label is None or self . label2token is None : raise ValueError ( \"This model is untrained.\" ) self . s4_model = self . s4_model . to ( self . device ) for module in self . s4_model . modules (): if hasattr ( module , \"setup_step\" ): module . setup_step () self . s4_model . eval () n_batches = math . ceil ( n_designs / batch_size ) designs , likelihoods = list (), list () for batch_idx in range ( n_batches ): if batch_idx == n_batches - 1 : batch_size = n_designs - batch_idx * batch_size X_test = ( torch . zeros ( batch_size , 1 ) . to ( torch . int ) + self . token2label [ \"[BEG]\" ] ) X_test = X_test . to ( self . device ) self . s4_model . reset_state ( batch_size , device = self . device ) X_test = X_test [:, 0 ] batch_designs , batch_likelihoods = list (), list () for __ in range ( self . sequence_length ): preds = self . s4_model . recurrent_step ( X_test ) softmax_preds = F . softmax ( preds , dim =- 1 ) . detach () . cpu () . numpy () . tolist () preds = preds . detach () . cpu () . numpy () . tolist () token_labels , token_likelihoods = list (), list () for pred_idx , pred in enumerate ( preds ): pred_temperature = np . exp ( np . array ( pred ) / temperature ) . tolist () pred_sum = sum ( pred_temperature ) pred_normed = [ p / pred_sum for p in pred_temperature ] probas = np . random . multinomial ( 1 , pred_normed ) token_label = np . argmax ( probas ) token_labels . append ( token_label ) token_likelihood = softmax_preds [ pred_idx ][ token_label ] token_likelihoods . append ( token_likelihood ) batch_designs . append ( token_labels ) batch_likelihoods . append ( token_likelihoods ) X_test = torch . tensor ( token_labels ) . to ( self . device ) designs . append ( np . array ( batch_designs ) . T ) likelihoods . append ( np . array ( batch_likelihoods ) . T ) designs = np . concatenate ( designs , axis = 0 ) . tolist () molecules = [ [ self . label2token [ label ] for label in design if self . label2token [ label ] not in [ \"[BEG]\" , \"[END]\" , \"[PAD]\" ] ] for design in designs ] molecule_lens = [ len ( molecule ) + 2 for molecule in molecules ] # +2 for [BEG] and [END] smiles = [ \"\" . join ( molecule ) for molecule in molecules ] loglikelihoods = np . log ( np . concatenate ( likelihoods , axis = 0 )) . tolist () mean_loglikelihoods = [ np . mean ( ll [: mol_len - 1 ]) for ll , mol_len in zip ( loglikelihoods , molecule_lens ) ] return smiles , mean_loglikelihoods @torch . no_grad () def compute_molecule_loglikelihoods ( self , molecules : List [ List [ str ]], batch_size : int ) -> List [ float ]: \"\"\"Computes the log-likelihoods of a list of molecules. The molecules are processed in batches of size `batch_size`. The log-likelihoods are returned as a list. Parameters ---------- molecules : List[List[str]] A list of SMILES strings. The input molecules are tokenized and padded (or truncated) internally to the sequence length used during training. batch_size : int The batch size to use during computation. Returns ------- List[float] A list of log-likelihoods. \"\"\" tokenized_molecules = [ [ \"[BEG]\" ] + smiles_utils . segment_smiles ( smiles ) + [ \"[END]\" ] for smiles in molecules ] padded_molecules = smiles_utils . pad_sequences ( tokenized_molecules , self . sequence_length + 1 , padding_value = \"[PAD]\" ) label_encoded_molecules = [ [ self . token2label [ token ] for token in tokens ] for tokens in padded_molecules ] self . s4_model = self . s4_model . to ( self . device ) for module in self . s4_model . modules (): if hasattr ( module , \"setup_step\" ): module . setup_step () self . s4_model . eval () n_batches = math . ceil ( len ( molecules ) / batch_size ) all_sequence_loglikelihoods = list () for batch_idx in range ( n_batches ): batch_start_idx = batch_idx * batch_size batch_end_idx = ( batch_idx + 1 ) * batch_size molecule_batch = label_encoded_molecules [ batch_start_idx : batch_end_idx ] self . s4_model . reset_state ( batch_size = len ( molecule_batch ), device = self . device ) batch_loglikelihoods = list () for label_idx in range ( self . sequence_length ): labels = [ molecule [ label_idx ] for molecule in molecule_batch ] X_test = torch . tensor ( labels , dtype = torch . int ) . to ( self . device ) preds = self . s4_model . recurrent_step ( X_test ) softmax_preds = F . softmax ( preds , dim =- 1 ) . detach () . cpu () . numpy () . tolist () log_preds = np . log ( softmax_preds ) next_token_labels = [ molecule [ label_idx + 1 ] for molecule in molecule_batch ] log_likelihoods = [ log_pred [ nt_label ] for nt_label , log_pred in zip ( next_token_labels , log_preds ) ] batch_loglikelihoods . append ( log_likelihoods ) batch_loglikelihoods = np . array ( batch_loglikelihoods ) . T . tolist () molecule_lengths = [ len ( molecule ) for molecule in tokenized_molecules [ batch_start_idx : batch_end_idx ] ] batch_sequence_loglikelihoods = [ np . mean ( ll [: mol_len - 1 ]) for ll , mol_len in zip ( batch_loglikelihoods , molecule_lengths ) ] all_sequence_loglikelihoods . extend ( batch_sequence_loglikelihoods ) return all_sequence_loglikelihoods def save ( self , path : str ): \"\"\"Saves the model to a directory. The directory will be created if it does not exist. Parameters ---------- path : str The directory to save the model to. \"\"\" print ( \"Saving model to\" , path ) os . makedirs ( path , exist_ok = True ) torch . save ( self . s4_model . state_dict (), f \" { path } /model.pt\" ) properties = { p : v for p , v in self . __dict__ . items () if p != \"s4_model\" } with open ( f \" { path } /init_arguments.json\" , \"w\" ) as f : json . dump ( properties , f , indent = 4 )","title":"S4forDenovoDesign"},{"location":"api/s4_for_denovo_design/#s4dd.s4_for_denovo_design.S4forDenovoDesign.__init__","text":"Creates an S4forDenovoDesign instance. The default configurations are the ones used in the paper . Parameters: Name Type Description Default model_dim int The number of dimensions used across the model. 256 state_dim int The dimension of the state in the recurrent mode. 64 n_layers int The number of S4 layers in the model. 4 n_ssm int The number of state space models in each layer. 1 dropout float The dropout rate. 0.25 vocab_size int The size of the vocabulary. 37 sequence_length int The length of the sequences. 99 n_max_epochs int The maximum number of epochs to train for. 400 learning_rate float The learning rate. 0.001 batch_size int The batch size. 2048 device str The device to put the model on, e.g., \"cuda\" or \"cpu\" . 'cuda' Source code in s4dd/s4_for_denovo_design.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 def __init__ ( self , model_dim : int = 256 , state_dim : int = 64 , n_layers : int = 4 , n_ssm : int = 1 , dropout : float = 0.25 , vocab_size : int = 37 , sequence_length : int = 99 , n_max_epochs : int = 400 , learning_rate : float = 0.001 , batch_size : int = 2048 , device : str = \"cuda\" , ) -> None : \"\"\"Creates an `S4forDenovoDesign` instance. The default configurations are the ones used in the [paper](https://chemrxiv.org/engage/chemrxiv/article-details/65168004ade1178b24567cd3). Parameters ---------- model_dim : int The number of dimensions used across the model. state_dim : int The dimension of the state in the recurrent mode. n_layers : int The number of S4 layers in the model. n_ssm : int The number of state space models in each layer. dropout : float The dropout rate. vocab_size : int The size of the vocabulary. sequence_length : int The length of the sequences. n_max_epochs : int The maximum number of epochs to train for. learning_rate : float The learning rate. batch_size : int The batch size. device : str The device to put the model on, *e.g.,* `\"cuda\"` or `\"cpu\"`. \"\"\" self . model_dim = model_dim self . state_dim = state_dim self . n_layers = n_layers self . n_ssm = n_ssm self . dropout = dropout self . vocab_size = vocab_size self . sequence_length = sequence_length self . n_max_epochs = n_max_epochs self . learning_rate = learning_rate self . batch_size = batch_size self . device = device # These are set during training self . token2label = None self . label2token = None self . s4_model = StructuredStateSpaceSequenceModel ( model_dim = self . model_dim , state_dim = self . state_dim , n_layers = self . n_layers , n_ssm = self . n_ssm , dropout = self . dropout , learning_rate = self . learning_rate , sequence_length = self . sequence_length , vocab_size = self . vocab_size , )","title":"__init__()"},{"location":"api/s4_for_denovo_design/#s4dd.s4_for_denovo_design.S4forDenovoDesign.compute_molecule_loglikelihoods","text":"Computes the log-likelihoods of a list of molecules. The molecules are processed in batches of size batch_size . The log-likelihoods are returned as a list. Parameters: Name Type Description Default molecules List [ List [ str ]] A list of SMILES strings. The input molecules are tokenized and padded (or truncated) internally to the sequence length used during training. required batch_size int The batch size to use during computation. required Returns: Type Description List [ float ] A list of log-likelihoods. Source code in s4dd/s4_for_denovo_design.py 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 @torch . no_grad () def compute_molecule_loglikelihoods ( self , molecules : List [ List [ str ]], batch_size : int ) -> List [ float ]: \"\"\"Computes the log-likelihoods of a list of molecules. The molecules are processed in batches of size `batch_size`. The log-likelihoods are returned as a list. Parameters ---------- molecules : List[List[str]] A list of SMILES strings. The input molecules are tokenized and padded (or truncated) internally to the sequence length used during training. batch_size : int The batch size to use during computation. Returns ------- List[float] A list of log-likelihoods. \"\"\" tokenized_molecules = [ [ \"[BEG]\" ] + smiles_utils . segment_smiles ( smiles ) + [ \"[END]\" ] for smiles in molecules ] padded_molecules = smiles_utils . pad_sequences ( tokenized_molecules , self . sequence_length + 1 , padding_value = \"[PAD]\" ) label_encoded_molecules = [ [ self . token2label [ token ] for token in tokens ] for tokens in padded_molecules ] self . s4_model = self . s4_model . to ( self . device ) for module in self . s4_model . modules (): if hasattr ( module , \"setup_step\" ): module . setup_step () self . s4_model . eval () n_batches = math . ceil ( len ( molecules ) / batch_size ) all_sequence_loglikelihoods = list () for batch_idx in range ( n_batches ): batch_start_idx = batch_idx * batch_size batch_end_idx = ( batch_idx + 1 ) * batch_size molecule_batch = label_encoded_molecules [ batch_start_idx : batch_end_idx ] self . s4_model . reset_state ( batch_size = len ( molecule_batch ), device = self . device ) batch_loglikelihoods = list () for label_idx in range ( self . sequence_length ): labels = [ molecule [ label_idx ] for molecule in molecule_batch ] X_test = torch . tensor ( labels , dtype = torch . int ) . to ( self . device ) preds = self . s4_model . recurrent_step ( X_test ) softmax_preds = F . softmax ( preds , dim =- 1 ) . detach () . cpu () . numpy () . tolist () log_preds = np . log ( softmax_preds ) next_token_labels = [ molecule [ label_idx + 1 ] for molecule in molecule_batch ] log_likelihoods = [ log_pred [ nt_label ] for nt_label , log_pred in zip ( next_token_labels , log_preds ) ] batch_loglikelihoods . append ( log_likelihoods ) batch_loglikelihoods = np . array ( batch_loglikelihoods ) . T . tolist () molecule_lengths = [ len ( molecule ) for molecule in tokenized_molecules [ batch_start_idx : batch_end_idx ] ] batch_sequence_loglikelihoods = [ np . mean ( ll [: mol_len - 1 ]) for ll , mol_len in zip ( batch_loglikelihoods , molecule_lengths ) ] all_sequence_loglikelihoods . extend ( batch_sequence_loglikelihoods ) return all_sequence_loglikelihoods","title":"compute_molecule_loglikelihoods()"},{"location":"api/s4_for_denovo_design/#s4dd.s4_for_denovo_design.S4forDenovoDesign.design_molecules","text":"Designs molecules using the trained model. The number of designs to generate is specified by n_designs . The designs are generated in batches of size batch_size . The temperature is used to control the diversity of the generated designs. The designs and their log-likelihoods are returned as a tuple. Parameters: Name Type Description Default n_designs int The number of designs to generate. required batch_size int The batch size to use during generation. required temperature float The temperature to use during generation. required Returns: Type Description Tuple [ List [ str ], List [ float ]] A tuple containing the generated SMILES strings and their log-likelihoods. Source code in s4dd/s4_for_denovo_design.py 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 @torch . no_grad () def design_molecules ( self , n_designs : int , batch_size : int , temperature : float , ) -> Tuple [ List [ str ], List [ float ]]: \"\"\"Designs molecules using the trained model. The number of designs to generate is specified by `n_designs`. The designs are generated in batches of size `batch_size`. The temperature is used to control the diversity of the generated designs. The designs and their log-likelihoods are returned as a tuple. Parameters ---------- n_designs : int The number of designs to generate. batch_size : int The batch size to use during generation. temperature : float The temperature to use during generation. Returns ------- Tuple[List[str], List[float]] A tuple containing the generated SMILES strings and their log-likelihoods. \"\"\" if self . token2label is None or self . label2token is None : raise ValueError ( \"This model is untrained.\" ) self . s4_model = self . s4_model . to ( self . device ) for module in self . s4_model . modules (): if hasattr ( module , \"setup_step\" ): module . setup_step () self . s4_model . eval () n_batches = math . ceil ( n_designs / batch_size ) designs , likelihoods = list (), list () for batch_idx in range ( n_batches ): if batch_idx == n_batches - 1 : batch_size = n_designs - batch_idx * batch_size X_test = ( torch . zeros ( batch_size , 1 ) . to ( torch . int ) + self . token2label [ \"[BEG]\" ] ) X_test = X_test . to ( self . device ) self . s4_model . reset_state ( batch_size , device = self . device ) X_test = X_test [:, 0 ] batch_designs , batch_likelihoods = list (), list () for __ in range ( self . sequence_length ): preds = self . s4_model . recurrent_step ( X_test ) softmax_preds = F . softmax ( preds , dim =- 1 ) . detach () . cpu () . numpy () . tolist () preds = preds . detach () . cpu () . numpy () . tolist () token_labels , token_likelihoods = list (), list () for pred_idx , pred in enumerate ( preds ): pred_temperature = np . exp ( np . array ( pred ) / temperature ) . tolist () pred_sum = sum ( pred_temperature ) pred_normed = [ p / pred_sum for p in pred_temperature ] probas = np . random . multinomial ( 1 , pred_normed ) token_label = np . argmax ( probas ) token_labels . append ( token_label ) token_likelihood = softmax_preds [ pred_idx ][ token_label ] token_likelihoods . append ( token_likelihood ) batch_designs . append ( token_labels ) batch_likelihoods . append ( token_likelihoods ) X_test = torch . tensor ( token_labels ) . to ( self . device ) designs . append ( np . array ( batch_designs ) . T ) likelihoods . append ( np . array ( batch_likelihoods ) . T ) designs = np . concatenate ( designs , axis = 0 ) . tolist () molecules = [ [ self . label2token [ label ] for label in design if self . label2token [ label ] not in [ \"[BEG]\" , \"[END]\" , \"[PAD]\" ] ] for design in designs ] molecule_lens = [ len ( molecule ) + 2 for molecule in molecules ] # +2 for [BEG] and [END] smiles = [ \"\" . join ( molecule ) for molecule in molecules ] loglikelihoods = np . log ( np . concatenate ( likelihoods , axis = 0 )) . tolist () mean_loglikelihoods = [ np . mean ( ll [: mol_len - 1 ]) for ll , mol_len in zip ( loglikelihoods , molecule_lens ) ] return smiles , mean_loglikelihoods","title":"design_molecules()"},{"location":"api/s4_for_denovo_design/#s4dd.s4_for_denovo_design.S4forDenovoDesign.from_file","text":"Loads an S4forDenovoDesign instance from a directory. Parameters: Name Type Description Default loaddir str The directory to load the model from. required Returns: Type Description S4forDenovoDesign The loaded model. Source code in s4dd/s4_for_denovo_design.py 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 @classmethod def from_file ( cls , loaddir : str ): \"\"\"Loads an `S4forDenovoDesign` instance from a directory. Parameters ---------- loaddir : str The directory to load the model from. Returns ------- S4forDenovoDesign The loaded model. \"\"\" with open ( f \" { loaddir } /init_arguments.json\" , \"r\" ) as f : properties = json . load ( f ) s4_model = StructuredStateSpaceSequenceModel ( model_dim = properties [ \"model_dim\" ], state_dim = properties [ \"state_dim\" ], n_layers = properties [ \"n_layers\" ], n_ssm = properties [ \"n_ssm\" ], dropout = properties [ \"dropout\" ], learning_rate = properties [ \"learning_rate\" ], sequence_length = properties [ \"sequence_length\" ], vocab_size = properties [ \"vocab_size\" ], ) s4_model . load_state_dict ( torch . load ( f \" { loaddir } /model.pt\" )) token2label = properties . pop ( \"token2label\" ) label2token = properties . pop ( \"label2token\" ) instance = cls ( ** properties ) instance . s4_model = s4_model instance . s4_model . to ( instance . device ) instance . token2label = token2label instance . label2token = { int ( label ): token for label , token in label2token . items () } return instance","title":"from_file()"},{"location":"api/s4_for_denovo_design/#s4dd.s4_for_denovo_design.S4forDenovoDesign.save","text":"Saves the model to a directory. The directory will be created if it does not exist. Parameters: Name Type Description Default path str The directory to save the model to. required Source code in s4dd/s4_for_denovo_design.py 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 def save ( self , path : str ): \"\"\"Saves the model to a directory. The directory will be created if it does not exist. Parameters ---------- path : str The directory to save the model to. \"\"\" print ( \"Saving model to\" , path ) os . makedirs ( path , exist_ok = True ) torch . save ( self . s4_model . state_dict (), f \" { path } /model.pt\" ) properties = { p : v for p , v in self . __dict__ . items () if p != \"s4_model\" } with open ( f \" { path } /init_arguments.json\" , \"w\" ) as f : json . dump ( properties , f , indent = 4 )","title":"save()"},{"location":"api/s4_for_denovo_design/#s4dd.s4_for_denovo_design.S4forDenovoDesign.train","text":"Trains the model. The inputs are the paths to the training and validation molecules. The paths should point either to a .txt file that contains one SMILES per line, or to a zip file with the same structure. The optional callbacks can be used to monitor or configure training. The training history is returned as a dictionary. Parameters: Name Type Description Default training_molecules_path str The path to the training molecules. Can be a zip file or a text file. Must contain one SMILES string per line. required val_molecules_path str The path to the validation molecules. Must have the same structure as training_molecules_path . required callbacks List [ torch_callbacks . TorchCallback ], optional A list of callbacks to use during training. See the documentation of the torch_callbacks module for available options. None Returns: Type Description Dict [ str , List [ float ]] A dictionary containing the training history. The keys are train_loss and val_loss and the values are lists of the metric values at each epoch. Source code in s4dd/s4_for_denovo_design.py 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 def train ( self , training_molecules_path : str , val_molecules_path : str , callbacks : List [ torch_callbacks . TorchCallback ] = None , ) -> Dict [ str , List [ float ]]: \"\"\"Trains the model. The inputs are the paths to the training and validation molecules. The paths should point either to a .txt file that contains one SMILES per line, or to a zip file with the same structure. The optional callbacks can be used to monitor or configure training. The training history is returned as a dictionary. Parameters ---------- training_molecules_path : str The path to the training molecules. Can be a zip file or a text file. Must contain one SMILES string per line. val_molecules_path : str The path to the validation molecules. Must have the same structure as `training_molecules_path`. callbacks : List[torch_callbacks.TorchCallback], optional A list of callbacks to use during training. See the documentation of the `torch_callbacks` module for available options. Returns ------- Dict[str, List[float]] A dictionary containing the training history. The keys are `train_loss` and `val_loss` and the values are lists of the metric values at each epoch. \"\"\" self . s4_model = self . s4_model . to ( self . device ) train_dataloader = create_dataloader ( training_molecules_path , batch_size = self . batch_size , sequence_length = self . sequence_length + 1 , num_workers = 1 , shuffle = True , token2label = self . token2label , ) self . token2label = train_dataloader . dataset . token2label self . label2token = { v : k for k , v in self . token2label . items ()} val_dataloader = create_dataloader ( val_molecules_path , batch_size = self . batch_size , sequence_length = self . sequence_length + 1 , num_workers = 1 , shuffle = True , token2label = self . token2label , ) loss_fn = nn . CrossEntropyLoss () optimizer = torch . optim . Adam ( self . s4_model . parameters (), lr = self . learning_rate ) history = { \"train_loss\" : list (), \"val_loss\" : list ()} epoch_train_loss = 0 for epoch_ix in range ( self . n_max_epochs ): self . s4_model . recurrent_state = None # Training self . s4_model . train () n_train_batches = len ( train_dataloader ) epoch_train_loss = 0 for X_train , y_train in tqdm . tqdm ( train_dataloader ): optimizer . zero_grad () batch_train_loss = self . _compute_loss ( loss_fn , X_train , y_train ) epoch_train_loss += batch_train_loss . item () batch_train_loss . backward () optimizer . step () epoch_train_loss = epoch_train_loss / n_train_batches history [ \"train_loss\" ] . append ( epoch_train_loss ) # Validation self . s4_model . eval () n_val_batches = len ( val_dataloader ) epoch_val_loss = 0 for X_val , y_val in val_dataloader : batch_val_loss = self . _compute_loss ( loss_fn , X_val , y_val ) epoch_val_loss += batch_val_loss . item () epoch_val_loss = epoch_val_loss / n_val_batches history [ \"val_loss\" ] . append ( epoch_val_loss ) # Callbacks print ( f \"Epoch: { epoch_ix } \\t Loss: { epoch_train_loss } , Val Loss: { epoch_val_loss } \" ) if callbacks is not None : for callback in callbacks : callback . on_epoch_end ( epoch_ix = epoch_ix , history = history ) stop_training_flags = [ callback . stop_training for callback in callbacks ] stop_training = sum ( stop_training_flags ) > 0 if stop_training : print ( \"Training stopped early. Epoch:\" , epoch_ix ) break if np . isnan ( epoch_train_loss ) or np . isnan ( epoch_val_loss ): print ( \"Training diverged. Epoch:\" , epoch_ix ) break if callbacks is not None : for callback in callbacks : callback . on_train_end ( epoch_ix = epoch_ix , history = history ) return history","title":"train()"},{"location":"api/s4_for_denovo_design/#s4dd.s4_for_denovo_design.StructuredStateSpaceSequenceModel","text":"Bases: nn . Module A general purpose structured state space sequence (S4) model implemented as a pytorch module. Source code in s4dd/s4_for_denovo_design.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 class StructuredStateSpaceSequenceModel ( nn . Module ): \"\"\"A general purpose structured state space sequence (S4) model implemented as a pytorch module.\"\"\" def __init__ ( self , model_dim : int , state_dim : int , n_layers : int , n_ssm : int , dropout : float , learning_rate : float , sequence_length : int , vocab_size : int , ) -> None : \"\"\"Creates a `StructuredStateSpaceSequenceModel` instance. Parameters ---------- model_dim : int The dimension of the model. state_dim : int The dimension of the state in recurrent mode. n_layers : int The number of S4 layers in the model. n_ssm : int The number of state space models in each layer. dropout : float The dropout rate. learning_rate : float The learning rate. sequence_length : int The length of the sequences. vocab_size : int The size of the vocabulary. \"\"\" super () . __init__ () self . model_dim = model_dim self . state_dim = state_dim self . n_layers = n_layers self . n_ssm = n_ssm self . dropout = dropout self . learning_rate = learning_rate self . sequence_length = sequence_length self . vocab_size = vocab_size self . layer_config = [ { \"_name_\" : \"s4\" , \"d_state\" : self . state_dim , \"n_ssm\" : self . n_ssm , }, { \"_name_\" : \"s4\" , \"d_state\" : self . state_dim , \"n_ssm\" : self . n_ssm , }, { \"_name_\" : \"ff\" }, ] self . pool_config = { \"_name_\" : \"pool\" , \"stride\" : 1 , \"expand\" : None } self . embedding = nn . Embedding ( self . vocab_size , self . model_dim ) self . model = SequenceModel ( d_model = self . model_dim , n_layers = self . n_layers , transposed = True , dropout = self . dropout , layer = self . layer_config , pool = self . pool_config , ) self . output_embedding = nn . Linear ( self . model_dim , self . vocab_size ) self . recurrent_state = None def forward ( self , batch : torch . Tensor ) -> torch . Tensor : \"\"\"Computes the forward pass of the model. The forward pass consists of embedding the input tokens, passing the embeddings through the S4 model (in convolutional mode), and then passing the output of the S4 model through a linear layer to get the logits. Parameters ---------- batch : torch.Tensor A batch of sequences of integers representing the tokens. The input shape is (batch_size, sequence_length, 1). Returns ------- torch.Tensor The logits of the model. \"\"\" batch = self . embedding ( batch ) batch = batch . view ( batch . shape [ 0 ], self . sequence_length , self . model_dim ) batch , state = self . model ( batch , state = self . recurrent_state ) self . recurrent_state = state batch = self . output_embedding ( batch ) return batch def reset_state ( self , batch_size : int , device : str = None ) -> None : \"\"\"Resets the recurrent state of the model. Used in sequential mode before processing a new batch. Parameters ---------- batch_size : int The batch size. device : str The device to put the state on, *e.g.,* `\"cuda\"` or `\"cpu\"`. \"\"\" self . recurrent_state = self . model . default_state ( batch_size , device = device ) def recurrent_step ( self , x_t ): \"\"\"Computes a single step in the recurrent mode. The internal state of the model is also updated. Parameters ---------- x_t : torch.Tensor The input token. The input shape is (batch_size, 1). Returns ------- torch.Tensor The logits resulting from the stepping. \"\"\" x_t = self . embedding ( x_t ) . view ( x_t . shape [ 0 ], 1 , self . model_dim ) x_t = x_t . squeeze ( 1 ) x_t , state = self . model . step ( x_t , state = self . recurrent_state ) self . recurrent_state = state x_t = self . output_embedding ( x_t ) return x_t","title":"StructuredStateSpaceSequenceModel"},{"location":"api/s4_for_denovo_design/#s4dd.s4_for_denovo_design.StructuredStateSpaceSequenceModel.__init__","text":"Creates a StructuredStateSpaceSequenceModel instance. Parameters: Name Type Description Default model_dim int The dimension of the model. required state_dim int The dimension of the state in recurrent mode. required n_layers int The number of S4 layers in the model. required n_ssm int The number of state space models in each layer. required dropout float The dropout rate. required learning_rate float The learning rate. required sequence_length int The length of the sequences. required vocab_size int The size of the vocabulary. required Source code in s4dd/s4_for_denovo_design.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def __init__ ( self , model_dim : int , state_dim : int , n_layers : int , n_ssm : int , dropout : float , learning_rate : float , sequence_length : int , vocab_size : int , ) -> None : \"\"\"Creates a `StructuredStateSpaceSequenceModel` instance. Parameters ---------- model_dim : int The dimension of the model. state_dim : int The dimension of the state in recurrent mode. n_layers : int The number of S4 layers in the model. n_ssm : int The number of state space models in each layer. dropout : float The dropout rate. learning_rate : float The learning rate. sequence_length : int The length of the sequences. vocab_size : int The size of the vocabulary. \"\"\" super () . __init__ () self . model_dim = model_dim self . state_dim = state_dim self . n_layers = n_layers self . n_ssm = n_ssm self . dropout = dropout self . learning_rate = learning_rate self . sequence_length = sequence_length self . vocab_size = vocab_size self . layer_config = [ { \"_name_\" : \"s4\" , \"d_state\" : self . state_dim , \"n_ssm\" : self . n_ssm , }, { \"_name_\" : \"s4\" , \"d_state\" : self . state_dim , \"n_ssm\" : self . n_ssm , }, { \"_name_\" : \"ff\" }, ] self . pool_config = { \"_name_\" : \"pool\" , \"stride\" : 1 , \"expand\" : None } self . embedding = nn . Embedding ( self . vocab_size , self . model_dim ) self . model = SequenceModel ( d_model = self . model_dim , n_layers = self . n_layers , transposed = True , dropout = self . dropout , layer = self . layer_config , pool = self . pool_config , ) self . output_embedding = nn . Linear ( self . model_dim , self . vocab_size ) self . recurrent_state = None","title":"__init__()"},{"location":"api/s4_for_denovo_design/#s4dd.s4_for_denovo_design.StructuredStateSpaceSequenceModel.forward","text":"Computes the forward pass of the model. The forward pass consists of embedding the input tokens, passing the embeddings through the S4 model (in convolutional mode), and then passing the output of the S4 model through a linear layer to get the logits. Parameters: Name Type Description Default batch torch . Tensor A batch of sequences of integers representing the tokens. The input shape is (batch_size, sequence_length, 1). required Returns: Type Description torch . Tensor The logits of the model. Source code in s4dd/s4_for_denovo_design.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def forward ( self , batch : torch . Tensor ) -> torch . Tensor : \"\"\"Computes the forward pass of the model. The forward pass consists of embedding the input tokens, passing the embeddings through the S4 model (in convolutional mode), and then passing the output of the S4 model through a linear layer to get the logits. Parameters ---------- batch : torch.Tensor A batch of sequences of integers representing the tokens. The input shape is (batch_size, sequence_length, 1). Returns ------- torch.Tensor The logits of the model. \"\"\" batch = self . embedding ( batch ) batch = batch . view ( batch . shape [ 0 ], self . sequence_length , self . model_dim ) batch , state = self . model ( batch , state = self . recurrent_state ) self . recurrent_state = state batch = self . output_embedding ( batch ) return batch","title":"forward()"},{"location":"api/s4_for_denovo_design/#s4dd.s4_for_denovo_design.StructuredStateSpaceSequenceModel.recurrent_step","text":"Computes a single step in the recurrent mode. The internal state of the model is also updated. Parameters: Name Type Description Default x_t torch . Tensor The input token. The input shape is (batch_size, 1). required Returns: Type Description torch . Tensor The logits resulting from the stepping. Source code in s4dd/s4_for_denovo_design.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def recurrent_step ( self , x_t ): \"\"\"Computes a single step in the recurrent mode. The internal state of the model is also updated. Parameters ---------- x_t : torch.Tensor The input token. The input shape is (batch_size, 1). Returns ------- torch.Tensor The logits resulting from the stepping. \"\"\" x_t = self . embedding ( x_t ) . view ( x_t . shape [ 0 ], 1 , self . model_dim ) x_t = x_t . squeeze ( 1 ) x_t , state = self . model . step ( x_t , state = self . recurrent_state ) self . recurrent_state = state x_t = self . output_embedding ( x_t ) return x_t","title":"recurrent_step()"},{"location":"api/s4_for_denovo_design/#s4dd.s4_for_denovo_design.StructuredStateSpaceSequenceModel.reset_state","text":"Resets the recurrent state of the model. Used in sequential mode before processing a new batch. Parameters: Name Type Description Default batch_size int The batch size. required device str The device to put the state on, e.g., \"cuda\" or \"cpu\" . None Source code in s4dd/s4_for_denovo_design.py 111 112 113 114 115 116 117 118 119 120 121 122 def reset_state ( self , batch_size : int , device : str = None ) -> None : \"\"\"Resets the recurrent state of the model. Used in sequential mode before processing a new batch. Parameters ---------- batch_size : int The batch size. device : str The device to put the state on, *e.g.,* `\"cuda\"` or `\"cpu\"`. \"\"\" self . recurrent_state = self . model . default_state ( batch_size , device = device )","title":"reset_state()"},{"location":"api/smiles_utils/","text":"smiles_utils learn_label_encoding ( tokenized_inputs ) Learn a label encoding from a tokenized dataset. The padding token, \"[PAD]\" is always assigned the label 0. Parameters: Name Type Description Default tokenized_inputs List [ List [ str ]] SMILES of the molecules in the dataset, tokenized into a list of tokens. required Returns: Type Description Dict [ str , int ] A dictionary mapping SMILES tokens to integer labels. Source code in s4dd/smiles_utils.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def learn_label_encoding ( tokenized_inputs : List [ List [ str ]]) -> Dict [ str , int ]: \"\"\"Learn a label encoding from a tokenized dataset. The padding token, `\"[PAD]\"` is always assigned the label 0. Parameters ---------- tokenized_inputs : List[List[str]] SMILES of the molecules in the dataset, tokenized into a list of tokens. Returns ------- Dict[str, int] A dictionary mapping SMILES tokens to integer labels. \"\"\" token2label = dict () token2label [ \"[PAD]\" ] = len ( token2label ) for inp in tokenized_inputs : for token in inp : if token not in token2label : token2label [ token ] = len ( token2label ) return token2label pad_sequences ( sequences , padding_length , padding_value ) Pad sequences to a given length. The padding is done at the end of the sequences. Longer sequences are truncated from the beginning. Parameters: Name Type Description Default sequences List[List[Union[str, int]] A list of sequences, either tokenized or label encoded SMILES. required padding_length int The length to pad the sequences to. required padding_value Union [ str , int ] The value to pad the sequences with. required Returns: Type Description List[List[Union[str, int]] The padded sequences. Source code in s4dd/smiles_utils.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def pad_sequences ( sequences : List [ List [ Union [ str , int ]]], padding_length : int , padding_value : Union [ str , int ], ) -> List [ List [ Union [ str , int ]]]: \"\"\"Pad sequences to a given length. The padding is done at the end of the sequences. Longer sequences are truncated from the beginning. Parameters ---------- sequences : List[List[Union[str, int]] A list of sequences, either tokenized or label encoded SMILES. padding_length : int The length to pad the sequences to. padding_value : Union[str, int] The value to pad the sequences with. Returns ------- List[List[Union[str, int]] The padded sequences. \"\"\" lens = [ len ( seq ) for seq in sequences ] diffs = [ max ( padding_length - len , 0 ) for len in lens ] padded_sequences = [ seq + [ padding_value ] * diff for seq , diff in zip ( sequences , diffs ) ] truncated_sequences = [ seq [ - padding_length :] for seq in padded_sequences ] return truncated_sequences segment_smiles ( smiles , segment_sq_brackets = True ) Segment a SMILES string into tokens. Parameters: Name Type Description Default smiles str A SMILES string. required segment_sq_brackets bool Whether to segment the square brackets \"[\" and \"]\" as tokens. The default is True . True Returns: Type Description List [ str ] A list of tokens. Source code in s4dd/smiles_utils.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def segment_smiles ( smiles : str , segment_sq_brackets = True ) -> List [ str ]: \"\"\"Segment a SMILES string into tokens. Parameters ---------- smiles : str A SMILES string. segment_sq_brackets : bool Whether to segment the square brackets `\"[\"` and `\"]\"` as tokens. The default is `True`. Returns ------- List[str] A list of tokens. \"\"\" regex = _RE_PATTERNS [ \"segmentation_sq\" ] if not segment_sq_brackets : regex = _RE_PATTERNS [ \"segmentation\" ] return regex . findall ( smiles ) segment_smiles_batch ( smiles_batch , segment_sq_brackets = True ) Segment a batch of SMILES strings into tokens. Parameters: Name Type Description Default smiles_batch List [ str ] A batch of SMILES strings. required segment_sq_brackets bool Whether to segment the square brackets \"[\" and \"]\" as tokens. The default is True . True Returns: Type Description List [ List [ str ]] A list of lists of tokens. Source code in s4dd/smiles_utils.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def segment_smiles_batch ( smiles_batch : List [ str ], segment_sq_brackets = True ) -> List [ List [ str ]]: \"\"\"Segment a batch of SMILES strings into tokens. Parameters ---------- smiles_batch : List[str] A batch of SMILES strings. segment_sq_brackets : bool Whether to segment the square brackets `\"[\"` and `\"]\"` as tokens. The default is `True`. Returns ------- List[List[str]] A list of lists of tokens. \"\"\" return [ segment_smiles ( smiles , segment_sq_brackets ) for smiles in smiles_batch ]","title":"smiles_utils"},{"location":"api/smiles_utils/#smiles_utils","text":"","title":"smiles_utils"},{"location":"api/smiles_utils/#s4dd.smiles_utils.learn_label_encoding","text":"Learn a label encoding from a tokenized dataset. The padding token, \"[PAD]\" is always assigned the label 0. Parameters: Name Type Description Default tokenized_inputs List [ List [ str ]] SMILES of the molecules in the dataset, tokenized into a list of tokens. required Returns: Type Description Dict [ str , int ] A dictionary mapping SMILES tokens to integer labels. Source code in s4dd/smiles_utils.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def learn_label_encoding ( tokenized_inputs : List [ List [ str ]]) -> Dict [ str , int ]: \"\"\"Learn a label encoding from a tokenized dataset. The padding token, `\"[PAD]\"` is always assigned the label 0. Parameters ---------- tokenized_inputs : List[List[str]] SMILES of the molecules in the dataset, tokenized into a list of tokens. Returns ------- Dict[str, int] A dictionary mapping SMILES tokens to integer labels. \"\"\" token2label = dict () token2label [ \"[PAD]\" ] = len ( token2label ) for inp in tokenized_inputs : for token in inp : if token not in token2label : token2label [ token ] = len ( token2label ) return token2label","title":"learn_label_encoding()"},{"location":"api/smiles_utils/#s4dd.smiles_utils.pad_sequences","text":"Pad sequences to a given length. The padding is done at the end of the sequences. Longer sequences are truncated from the beginning. Parameters: Name Type Description Default sequences List[List[Union[str, int]] A list of sequences, either tokenized or label encoded SMILES. required padding_length int The length to pad the sequences to. required padding_value Union [ str , int ] The value to pad the sequences with. required Returns: Type Description List[List[Union[str, int]] The padded sequences. Source code in s4dd/smiles_utils.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def pad_sequences ( sequences : List [ List [ Union [ str , int ]]], padding_length : int , padding_value : Union [ str , int ], ) -> List [ List [ Union [ str , int ]]]: \"\"\"Pad sequences to a given length. The padding is done at the end of the sequences. Longer sequences are truncated from the beginning. Parameters ---------- sequences : List[List[Union[str, int]] A list of sequences, either tokenized or label encoded SMILES. padding_length : int The length to pad the sequences to. padding_value : Union[str, int] The value to pad the sequences with. Returns ------- List[List[Union[str, int]] The padded sequences. \"\"\" lens = [ len ( seq ) for seq in sequences ] diffs = [ max ( padding_length - len , 0 ) for len in lens ] padded_sequences = [ seq + [ padding_value ] * diff for seq , diff in zip ( sequences , diffs ) ] truncated_sequences = [ seq [ - padding_length :] for seq in padded_sequences ] return truncated_sequences","title":"pad_sequences()"},{"location":"api/smiles_utils/#s4dd.smiles_utils.segment_smiles","text":"Segment a SMILES string into tokens. Parameters: Name Type Description Default smiles str A SMILES string. required segment_sq_brackets bool Whether to segment the square brackets \"[\" and \"]\" as tokens. The default is True . True Returns: Type Description List [ str ] A list of tokens. Source code in s4dd/smiles_utils.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def segment_smiles ( smiles : str , segment_sq_brackets = True ) -> List [ str ]: \"\"\"Segment a SMILES string into tokens. Parameters ---------- smiles : str A SMILES string. segment_sq_brackets : bool Whether to segment the square brackets `\"[\"` and `\"]\"` as tokens. The default is `True`. Returns ------- List[str] A list of tokens. \"\"\" regex = _RE_PATTERNS [ \"segmentation_sq\" ] if not segment_sq_brackets : regex = _RE_PATTERNS [ \"segmentation\" ] return regex . findall ( smiles )","title":"segment_smiles()"},{"location":"api/smiles_utils/#s4dd.smiles_utils.segment_smiles_batch","text":"Segment a batch of SMILES strings into tokens. Parameters: Name Type Description Default smiles_batch List [ str ] A batch of SMILES strings. required segment_sq_brackets bool Whether to segment the square brackets \"[\" and \"]\" as tokens. The default is True . True Returns: Type Description List [ List [ str ]] A list of lists of tokens. Source code in s4dd/smiles_utils.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def segment_smiles_batch ( smiles_batch : List [ str ], segment_sq_brackets = True ) -> List [ List [ str ]]: \"\"\"Segment a batch of SMILES strings into tokens. Parameters ---------- smiles_batch : List[str] A batch of SMILES strings. segment_sq_brackets : bool Whether to segment the square brackets `\"[\"` and `\"]\"` as tokens. The default is `True`. Returns ------- List[List[str]] A list of lists of tokens. \"\"\" return [ segment_smiles ( smiles , segment_sq_brackets ) for smiles in smiles_batch ]","title":"segment_smiles_batch()"},{"location":"api/torch_callbacks/","text":"torch_callbacks DenovoDesign Bases: TorchCallback A callback for de novo design that designs SMILES strings in the end of every epoch. Source code in s4dd/torch_callbacks.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 class DenovoDesign ( TorchCallback ): \"\"\"A callback for de novo design that designs SMILES strings in the end of every epoch.\"\"\" def __init__ ( self , design_fn : Callable [[ float ], List [ str ]], basedir : str , temperatures : List [ float ], ) -> None : \"\"\"Creates a `DenovoDesign` instance. Parameters ---------- design_fn : Callable[[float], List[str]] A function that takes a temperature and returns a list of SMILES strings. basedir : str The base directory to save the generated molecules to. temperatures : List[float] A list of temperatures to use for sampling. \"\"\" super () . __init__ () self . design_fn = design_fn self . basedir = basedir self . temperatures = temperatures def on_epoch_end ( self , epoch_ix , ** kwargs ) -> None : \"\"\"Designs and saves molecules in the end of every epoch with their log-likelihoods. Parameters ---------- epoch_ix : int The index of the epoch that just ended. \"\"\" epoch_ix = epoch_ix + 1 # switch to 1-indexing print ( \"Designing molecules. Epoch\" , epoch_ix ) epoch_dir = _SAVE_FORMAT . format ( basedir = self . basedir , epoch_ix = epoch_ix ) os . makedirs ( epoch_dir , exist_ok = True ) for temperature in self . temperatures : molecules , log_likelihoods = self . design_fn ( temperature ) with open ( f \" { epoch_dir } /designed_chemicals-T_ { temperature } .smiles\" , \"w\" ) as f : f . write ( \" \\n \" . join ( molecules )) np . savetxt ( f \" { epoch_dir } /designed_loglikelihoods-T_ { temperature } .csv\" , log_likelihoods , delimiter = \",\" , ) __init__ ( design_fn , basedir , temperatures ) Creates a DenovoDesign instance. Parameters: Name Type Description Default design_fn Callable [[ float ], List [ str ]] A function that takes a temperature and returns a list of SMILES strings. required basedir str The base directory to save the generated molecules to. required temperatures List [ float ] A list of temperatures to use for sampling. required Source code in s4dd/torch_callbacks.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def __init__ ( self , design_fn : Callable [[ float ], List [ str ]], basedir : str , temperatures : List [ float ], ) -> None : \"\"\"Creates a `DenovoDesign` instance. Parameters ---------- design_fn : Callable[[float], List[str]] A function that takes a temperature and returns a list of SMILES strings. basedir : str The base directory to save the generated molecules to. temperatures : List[float] A list of temperatures to use for sampling. \"\"\" super () . __init__ () self . design_fn = design_fn self . basedir = basedir self . temperatures = temperatures on_epoch_end ( epoch_ix , ** kwargs ) Designs and saves molecules in the end of every epoch with their log-likelihoods. Parameters: Name Type Description Default epoch_ix int The index of the epoch that just ended. required Source code in s4dd/torch_callbacks.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def on_epoch_end ( self , epoch_ix , ** kwargs ) -> None : \"\"\"Designs and saves molecules in the end of every epoch with their log-likelihoods. Parameters ---------- epoch_ix : int The index of the epoch that just ended. \"\"\" epoch_ix = epoch_ix + 1 # switch to 1-indexing print ( \"Designing molecules. Epoch\" , epoch_ix ) epoch_dir = _SAVE_FORMAT . format ( basedir = self . basedir , epoch_ix = epoch_ix ) os . makedirs ( epoch_dir , exist_ok = True ) for temperature in self . temperatures : molecules , log_likelihoods = self . design_fn ( temperature ) with open ( f \" { epoch_dir } /designed_chemicals-T_ { temperature } .smiles\" , \"w\" ) as f : f . write ( \" \\n \" . join ( molecules )) np . savetxt ( f \" { epoch_dir } /designed_loglikelihoods-T_ { temperature } .csv\" , log_likelihoods , delimiter = \",\" , ) EarlyStopping Bases: TorchCallback A callback that stops training when a monitored metric has stopped improving. Source code in s4dd/torch_callbacks.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 class EarlyStopping ( TorchCallback ): \"\"\"A callback that stops training when a monitored metric has stopped improving.\"\"\" def __init__ ( self , patience : int , delta : float , criterion : str , mode : str ) -> None : \"\"\"Creates an `EarlyStopping` callback. Parameters ---------- patience : int Number of epochs to wait for improvement before stopping the training. delta : float Minimum change in the monitored quantity to qualify as an improvement. criterion : str The name of the metric to monitor. mode : str One of `\"min\"` or `\"max\"`. In `\"min\"` mode, training will stop when the quantity monitored has stopped decreasing; in `\"max\"` mode it will stop when the quantity monitored has stopped increasing. \"\"\" super () . __init__ () self . patience = patience self . delta = delta self . criterion = criterion if mode not in [ \"min\" , \"max\" ]: raise ValueError ( f \"mode must be 'min' or 'max', got { mode } \" ) self . mode = mode self . best = np . inf if mode == \"min\" else - np . inf self . best_epoch = 0 self . wait = 0 self . stopped_epoch = 0 def on_epoch_end ( self , epoch_ix : int , history : Dict [ str , float ], ** kwargs ) -> None : \"\"\"Called at the end of an epoch. Updates the best metric value and the number of epochs waited for improvement. `stop_training` attribute is set to `True` if the training should be stopped. Parameters ---------- epoch_ix : int The index of the epoch that just ended. history : Dict[str, float] A dictionary containing the training history. The keys are the names of the metrics, and the values are lists of the metric values at each epoch. \"\"\" monitor_values = history [ self . criterion ] self . wait += 1 if len ( monitor_values ) < self . patience : return current = monitor_values [ epoch_ix ] if self . _is_improvement ( current ): self . best = current self . best_epoch = epoch_ix self . wait = 0 elif self . wait >= self . patience : self . stop_training = True self . stopped_epoch = epoch_ix def _is_improvement ( self , current ): if self . mode == \"min\" : return current < self . best - self . delta return current > self . best + self . delta __init__ ( patience , delta , criterion , mode ) Creates an EarlyStopping callback. Parameters: Name Type Description Default patience int Number of epochs to wait for improvement before stopping the training. required delta float Minimum change in the monitored quantity to qualify as an improvement. required criterion str The name of the metric to monitor. required mode str One of \"min\" or \"max\" . In \"min\" mode, training will stop when the quantity monitored has stopped decreasing; in \"max\" mode it will stop when the quantity monitored has stopped increasing. required Source code in s4dd/torch_callbacks.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def __init__ ( self , patience : int , delta : float , criterion : str , mode : str ) -> None : \"\"\"Creates an `EarlyStopping` callback. Parameters ---------- patience : int Number of epochs to wait for improvement before stopping the training. delta : float Minimum change in the monitored quantity to qualify as an improvement. criterion : str The name of the metric to monitor. mode : str One of `\"min\"` or `\"max\"`. In `\"min\"` mode, training will stop when the quantity monitored has stopped decreasing; in `\"max\"` mode it will stop when the quantity monitored has stopped increasing. \"\"\" super () . __init__ () self . patience = patience self . delta = delta self . criterion = criterion if mode not in [ \"min\" , \"max\" ]: raise ValueError ( f \"mode must be 'min' or 'max', got { mode } \" ) self . mode = mode self . best = np . inf if mode == \"min\" else - np . inf self . best_epoch = 0 self . wait = 0 self . stopped_epoch = 0 on_epoch_end ( epoch_ix , history , ** kwargs ) Called at the end of an epoch. Updates the best metric value and the number of epochs waited for improvement. stop_training attribute is set to True if the training should be stopped. Parameters: Name Type Description Default epoch_ix int The index of the epoch that just ended. required history Dict [ str , float ] A dictionary containing the training history. The keys are the names of the metrics, and the values are lists of the metric values at each epoch. required Source code in s4dd/torch_callbacks.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def on_epoch_end ( self , epoch_ix : int , history : Dict [ str , float ], ** kwargs ) -> None : \"\"\"Called at the end of an epoch. Updates the best metric value and the number of epochs waited for improvement. `stop_training` attribute is set to `True` if the training should be stopped. Parameters ---------- epoch_ix : int The index of the epoch that just ended. history : Dict[str, float] A dictionary containing the training history. The keys are the names of the metrics, and the values are lists of the metric values at each epoch. \"\"\" monitor_values = history [ self . criterion ] self . wait += 1 if len ( monitor_values ) < self . patience : return current = monitor_values [ epoch_ix ] if self . _is_improvement ( current ): self . best = current self . best_epoch = epoch_ix self . wait = 0 elif self . wait >= self . patience : self . stop_training = True self . stopped_epoch = epoch_ix HistoryLogger Bases: TorchCallback A callback that saves the training history in the end of every epoch. Source code in s4dd/torch_callbacks.py 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 class HistoryLogger ( TorchCallback ): \"\"\"A callback that saves the training history in the end of every epoch.\"\"\" def __init__ ( self , savedir : str ) -> None : \"\"\"Creates a `HistoryLogger` instance. Parameters ---------- savedir : str The directory to save the training history to. \"\"\" super () . __init__ () self . savedir = savedir os . makedirs ( self . savedir , exist_ok = True ) def on_epoch_end ( self , history : Dict [ str , List [ float ]], ** kwargs ) -> None : \"\"\"Saves the training history in the end of every epoch. Parameters ---------- history : Dict[str, List[float]] A dictionary containing the training history. The keys are the names of the metrics (`\"val_loss\"` and `\"train_loss\"`), and the values are lists of the metric values at each epoch. \"\"\" with open ( os . path . join ( self . savedir , \"history.json\" ), \"w\" ) as f : json . dump ( history , f , indent = 4 ) json . dump ( history , f , indent = 4 ) __init__ ( savedir ) Creates a HistoryLogger instance. Parameters: Name Type Description Default savedir str The directory to save the training history to. required Source code in s4dd/torch_callbacks.py 220 221 222 223 224 225 226 227 228 229 230 def __init__ ( self , savedir : str ) -> None : \"\"\"Creates a `HistoryLogger` instance. Parameters ---------- savedir : str The directory to save the training history to. \"\"\" super () . __init__ () self . savedir = savedir os . makedirs ( self . savedir , exist_ok = True ) on_epoch_end ( history , ** kwargs ) Saves the training history in the end of every epoch. Parameters: Name Type Description Default history Dict [ str , List [ float ]] A dictionary containing the training history. The keys are the names of the metrics ( \"val_loss\" and \"train_loss\" ), and the values are lists of the metric values at each epoch. required Source code in s4dd/torch_callbacks.py 232 233 234 235 236 237 238 239 240 241 242 def on_epoch_end ( self , history : Dict [ str , List [ float ]], ** kwargs ) -> None : \"\"\"Saves the training history in the end of every epoch. Parameters ---------- history : Dict[str, List[float]] A dictionary containing the training history. The keys are the names of the metrics (`\"val_loss\"` and `\"train_loss\"`), and the values are lists of the metric values at each epoch. \"\"\" with open ( os . path . join ( self . savedir , \"history.json\" ), \"w\" ) as f : json . dump ( history , f , indent = 4 ) json . dump ( history , f , indent = 4 ) ModelCheckpoint Bases: TorchCallback A callback that saves the model in the end of every epoch. Source code in s4dd/torch_callbacks.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 class ModelCheckpoint ( TorchCallback ): \"\"\"A callback that saves the model in the end of every epoch.\"\"\" def __init__ ( self , save_fn : Callable [[ str ], None ], save_per_epoch : int , basedir : str , ) -> None : \"\"\"Creates a `ModelCheckpoint` instance that runs per a fixed number of epoch and at the end of training. Parameters ---------- save_fn : Callable[[str], None] A function that takes a directory and saves the model to that directory. save_per_epoch : int The number of epochs to wait between saves. basedir : str The base directory to save the model to. \"\"\" super () . __init__ () self . save_fn = save_fn self . save_per_epoch = save_per_epoch self . basedir = basedir def _save ( self , epoch_ix : int , ** kwargs ) -> None : savedir = os . path . join ( self . basedir , f \"epoch- { epoch_ix : 03d } \" ) os . makedirs ( savedir , exist_ok = True ) self . save_fn ( savedir ) def on_epoch_end ( self , epoch_ix : int , ** kwargs ) -> None : \"\"\"Saves the model in the end of every epoch. Parameters ---------- epoch_ix : int The index of the epoch that just ended. \"\"\" epoch_ix = epoch_ix + 1 # 1-indexed if epoch_ix % self . save_per_epoch == 0 : self . _save ( epoch_ix ) def on_train_end ( self , epoch_ix : int , ** kwargs ) -> None : \"\"\"Saves the model in the end of training. Parameters ---------- epoch_ix : int The index of the epoch that just ended. \"\"\" self . _save ( epoch_ix + 1 ) __init__ ( save_fn , save_per_epoch , basedir ) Creates a ModelCheckpoint instance that runs per a fixed number of epoch and at the end of training. Parameters: Name Type Description Default save_fn Callable [[ str ], None] A function that takes a directory and saves the model to that directory. required save_per_epoch int The number of epochs to wait between saves. required basedir str The base directory to save the model to. required Source code in s4dd/torch_callbacks.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def __init__ ( self , save_fn : Callable [[ str ], None ], save_per_epoch : int , basedir : str , ) -> None : \"\"\"Creates a `ModelCheckpoint` instance that runs per a fixed number of epoch and at the end of training. Parameters ---------- save_fn : Callable[[str], None] A function that takes a directory and saves the model to that directory. save_per_epoch : int The number of epochs to wait between saves. basedir : str The base directory to save the model to. \"\"\" super () . __init__ () self . save_fn = save_fn self . save_per_epoch = save_per_epoch self . basedir = basedir on_epoch_end ( epoch_ix , ** kwargs ) Saves the model in the end of every epoch. Parameters: Name Type Description Default epoch_ix int The index of the epoch that just ended. required Source code in s4dd/torch_callbacks.py 193 194 195 196 197 198 199 200 201 202 203 204 def on_epoch_end ( self , epoch_ix : int , ** kwargs ) -> None : \"\"\"Saves the model in the end of every epoch. Parameters ---------- epoch_ix : int The index of the epoch that just ended. \"\"\" epoch_ix = epoch_ix + 1 # 1-indexed if epoch_ix % self . save_per_epoch == 0 : self . _save ( epoch_ix ) on_train_end ( epoch_ix , ** kwargs ) Saves the model in the end of training. Parameters: Name Type Description Default epoch_ix int The index of the epoch that just ended. required Source code in s4dd/torch_callbacks.py 206 207 208 209 210 211 212 213 214 def on_train_end ( self , epoch_ix : int , ** kwargs ) -> None : \"\"\"Saves the model in the end of training. Parameters ---------- epoch_ix : int The index of the epoch that just ended. \"\"\" self . _save ( epoch_ix + 1 ) TorchCallback Bases: ABC Base class for all Torch callbacks. Source code in s4dd/torch_callbacks.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 class TorchCallback ( ABC ): \"\"\"Base class for all Torch callbacks.\"\"\" def __init__ ( self ) -> None : \"\"\"Creates a TorchCallback. Sets the `stop_training` flag to `False`, which would be common attribute of all callbacks.\"\"\" super () . __init__ () self . stop_training = False def on_epoch_end ( self , epoch_ix , history , ** kwargs ): \"\"\"Called at the end of an epoch. Parameters ---------- epoch_ix : int The index of the epoch that just ended. history : Dict[str, List[float]] A dictionary containing the training history. The keys are the names of the metrics, and the values are lists of the metric values at each epoch. **kwargs Any additional keyword arguments. \"\"\" pass def on_train_end ( self , epoch_ix , history , ** kwargs ): \"\"\"Called at the end of training. Parameters ---------- epoch_ix : int The index of the epoch that just ended. history : Dict[str, List[float]] A dictionary containing the training history. The keys are the names of the metrics, and the values are lists of the metric values at each epoch. **kwargs Any additional keyword arguments. \"\"\" pass __init__ () Creates a TorchCallback. Sets the stop_training flag to False , which would be common attribute of all callbacks. Source code in s4dd/torch_callbacks.py 14 15 16 17 def __init__ ( self ) -> None : \"\"\"Creates a TorchCallback. Sets the `stop_training` flag to `False`, which would be common attribute of all callbacks.\"\"\" super () . __init__ () self . stop_training = False on_epoch_end ( epoch_ix , history , ** kwargs ) Called at the end of an epoch. Parameters: Name Type Description Default epoch_ix int The index of the epoch that just ended. required history Dict [ str , List [ float ]] A dictionary containing the training history. The keys are the names of the metrics, and the values are lists of the metric values at each epoch. required **kwargs Any additional keyword arguments. {} Source code in s4dd/torch_callbacks.py 19 20 21 22 23 24 25 26 27 28 29 30 31 def on_epoch_end ( self , epoch_ix , history , ** kwargs ): \"\"\"Called at the end of an epoch. Parameters ---------- epoch_ix : int The index of the epoch that just ended. history : Dict[str, List[float]] A dictionary containing the training history. The keys are the names of the metrics, and the values are lists of the metric values at each epoch. **kwargs Any additional keyword arguments. \"\"\" pass on_train_end ( epoch_ix , history , ** kwargs ) Called at the end of training. Parameters: Name Type Description Default epoch_ix int The index of the epoch that just ended. required history Dict [ str , List [ float ]] A dictionary containing the training history. The keys are the names of the metrics, and the values are lists of the metric values at each epoch. required **kwargs Any additional keyword arguments. {} Source code in s4dd/torch_callbacks.py 33 34 35 36 37 38 39 40 41 42 43 44 45 def on_train_end ( self , epoch_ix , history , ** kwargs ): \"\"\"Called at the end of training. Parameters ---------- epoch_ix : int The index of the epoch that just ended. history : Dict[str, List[float]] A dictionary containing the training history. The keys are the names of the metrics, and the values are lists of the metric values at each epoch. **kwargs Any additional keyword arguments. \"\"\" pass","title":"torch_callbacks"},{"location":"api/torch_callbacks/#torch_callbacks","text":"","title":"torch_callbacks"},{"location":"api/torch_callbacks/#s4dd.torch_callbacks.DenovoDesign","text":"Bases: TorchCallback A callback for de novo design that designs SMILES strings in the end of every epoch. Source code in s4dd/torch_callbacks.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 class DenovoDesign ( TorchCallback ): \"\"\"A callback for de novo design that designs SMILES strings in the end of every epoch.\"\"\" def __init__ ( self , design_fn : Callable [[ float ], List [ str ]], basedir : str , temperatures : List [ float ], ) -> None : \"\"\"Creates a `DenovoDesign` instance. Parameters ---------- design_fn : Callable[[float], List[str]] A function that takes a temperature and returns a list of SMILES strings. basedir : str The base directory to save the generated molecules to. temperatures : List[float] A list of temperatures to use for sampling. \"\"\" super () . __init__ () self . design_fn = design_fn self . basedir = basedir self . temperatures = temperatures def on_epoch_end ( self , epoch_ix , ** kwargs ) -> None : \"\"\"Designs and saves molecules in the end of every epoch with their log-likelihoods. Parameters ---------- epoch_ix : int The index of the epoch that just ended. \"\"\" epoch_ix = epoch_ix + 1 # switch to 1-indexing print ( \"Designing molecules. Epoch\" , epoch_ix ) epoch_dir = _SAVE_FORMAT . format ( basedir = self . basedir , epoch_ix = epoch_ix ) os . makedirs ( epoch_dir , exist_ok = True ) for temperature in self . temperatures : molecules , log_likelihoods = self . design_fn ( temperature ) with open ( f \" { epoch_dir } /designed_chemicals-T_ { temperature } .smiles\" , \"w\" ) as f : f . write ( \" \\n \" . join ( molecules )) np . savetxt ( f \" { epoch_dir } /designed_loglikelihoods-T_ { temperature } .csv\" , log_likelihoods , delimiter = \",\" , )","title":"DenovoDesign"},{"location":"api/torch_callbacks/#s4dd.torch_callbacks.DenovoDesign.__init__","text":"Creates a DenovoDesign instance. Parameters: Name Type Description Default design_fn Callable [[ float ], List [ str ]] A function that takes a temperature and returns a list of SMILES strings. required basedir str The base directory to save the generated molecules to. required temperatures List [ float ] A list of temperatures to use for sampling. required Source code in s4dd/torch_callbacks.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def __init__ ( self , design_fn : Callable [[ float ], List [ str ]], basedir : str , temperatures : List [ float ], ) -> None : \"\"\"Creates a `DenovoDesign` instance. Parameters ---------- design_fn : Callable[[float], List[str]] A function that takes a temperature and returns a list of SMILES strings. basedir : str The base directory to save the generated molecules to. temperatures : List[float] A list of temperatures to use for sampling. \"\"\" super () . __init__ () self . design_fn = design_fn self . basedir = basedir self . temperatures = temperatures","title":"__init__()"},{"location":"api/torch_callbacks/#s4dd.torch_callbacks.DenovoDesign.on_epoch_end","text":"Designs and saves molecules in the end of every epoch with their log-likelihoods. Parameters: Name Type Description Default epoch_ix int The index of the epoch that just ended. required Source code in s4dd/torch_callbacks.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def on_epoch_end ( self , epoch_ix , ** kwargs ) -> None : \"\"\"Designs and saves molecules in the end of every epoch with their log-likelihoods. Parameters ---------- epoch_ix : int The index of the epoch that just ended. \"\"\" epoch_ix = epoch_ix + 1 # switch to 1-indexing print ( \"Designing molecules. Epoch\" , epoch_ix ) epoch_dir = _SAVE_FORMAT . format ( basedir = self . basedir , epoch_ix = epoch_ix ) os . makedirs ( epoch_dir , exist_ok = True ) for temperature in self . temperatures : molecules , log_likelihoods = self . design_fn ( temperature ) with open ( f \" { epoch_dir } /designed_chemicals-T_ { temperature } .smiles\" , \"w\" ) as f : f . write ( \" \\n \" . join ( molecules )) np . savetxt ( f \" { epoch_dir } /designed_loglikelihoods-T_ { temperature } .csv\" , log_likelihoods , delimiter = \",\" , )","title":"on_epoch_end()"},{"location":"api/torch_callbacks/#s4dd.torch_callbacks.EarlyStopping","text":"Bases: TorchCallback A callback that stops training when a monitored metric has stopped improving. Source code in s4dd/torch_callbacks.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 class EarlyStopping ( TorchCallback ): \"\"\"A callback that stops training when a monitored metric has stopped improving.\"\"\" def __init__ ( self , patience : int , delta : float , criterion : str , mode : str ) -> None : \"\"\"Creates an `EarlyStopping` callback. Parameters ---------- patience : int Number of epochs to wait for improvement before stopping the training. delta : float Minimum change in the monitored quantity to qualify as an improvement. criterion : str The name of the metric to monitor. mode : str One of `\"min\"` or `\"max\"`. In `\"min\"` mode, training will stop when the quantity monitored has stopped decreasing; in `\"max\"` mode it will stop when the quantity monitored has stopped increasing. \"\"\" super () . __init__ () self . patience = patience self . delta = delta self . criterion = criterion if mode not in [ \"min\" , \"max\" ]: raise ValueError ( f \"mode must be 'min' or 'max', got { mode } \" ) self . mode = mode self . best = np . inf if mode == \"min\" else - np . inf self . best_epoch = 0 self . wait = 0 self . stopped_epoch = 0 def on_epoch_end ( self , epoch_ix : int , history : Dict [ str , float ], ** kwargs ) -> None : \"\"\"Called at the end of an epoch. Updates the best metric value and the number of epochs waited for improvement. `stop_training` attribute is set to `True` if the training should be stopped. Parameters ---------- epoch_ix : int The index of the epoch that just ended. history : Dict[str, float] A dictionary containing the training history. The keys are the names of the metrics, and the values are lists of the metric values at each epoch. \"\"\" monitor_values = history [ self . criterion ] self . wait += 1 if len ( monitor_values ) < self . patience : return current = monitor_values [ epoch_ix ] if self . _is_improvement ( current ): self . best = current self . best_epoch = epoch_ix self . wait = 0 elif self . wait >= self . patience : self . stop_training = True self . stopped_epoch = epoch_ix def _is_improvement ( self , current ): if self . mode == \"min\" : return current < self . best - self . delta return current > self . best + self . delta","title":"EarlyStopping"},{"location":"api/torch_callbacks/#s4dd.torch_callbacks.EarlyStopping.__init__","text":"Creates an EarlyStopping callback. Parameters: Name Type Description Default patience int Number of epochs to wait for improvement before stopping the training. required delta float Minimum change in the monitored quantity to qualify as an improvement. required criterion str The name of the metric to monitor. required mode str One of \"min\" or \"max\" . In \"min\" mode, training will stop when the quantity monitored has stopped decreasing; in \"max\" mode it will stop when the quantity monitored has stopped increasing. required Source code in s4dd/torch_callbacks.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def __init__ ( self , patience : int , delta : float , criterion : str , mode : str ) -> None : \"\"\"Creates an `EarlyStopping` callback. Parameters ---------- patience : int Number of epochs to wait for improvement before stopping the training. delta : float Minimum change in the monitored quantity to qualify as an improvement. criterion : str The name of the metric to monitor. mode : str One of `\"min\"` or `\"max\"`. In `\"min\"` mode, training will stop when the quantity monitored has stopped decreasing; in `\"max\"` mode it will stop when the quantity monitored has stopped increasing. \"\"\" super () . __init__ () self . patience = patience self . delta = delta self . criterion = criterion if mode not in [ \"min\" , \"max\" ]: raise ValueError ( f \"mode must be 'min' or 'max', got { mode } \" ) self . mode = mode self . best = np . inf if mode == \"min\" else - np . inf self . best_epoch = 0 self . wait = 0 self . stopped_epoch = 0","title":"__init__()"},{"location":"api/torch_callbacks/#s4dd.torch_callbacks.EarlyStopping.on_epoch_end","text":"Called at the end of an epoch. Updates the best metric value and the number of epochs waited for improvement. stop_training attribute is set to True if the training should be stopped. Parameters: Name Type Description Default epoch_ix int The index of the epoch that just ended. required history Dict [ str , float ] A dictionary containing the training history. The keys are the names of the metrics, and the values are lists of the metric values at each epoch. required Source code in s4dd/torch_callbacks.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def on_epoch_end ( self , epoch_ix : int , history : Dict [ str , float ], ** kwargs ) -> None : \"\"\"Called at the end of an epoch. Updates the best metric value and the number of epochs waited for improvement. `stop_training` attribute is set to `True` if the training should be stopped. Parameters ---------- epoch_ix : int The index of the epoch that just ended. history : Dict[str, float] A dictionary containing the training history. The keys are the names of the metrics, and the values are lists of the metric values at each epoch. \"\"\" monitor_values = history [ self . criterion ] self . wait += 1 if len ( monitor_values ) < self . patience : return current = monitor_values [ epoch_ix ] if self . _is_improvement ( current ): self . best = current self . best_epoch = epoch_ix self . wait = 0 elif self . wait >= self . patience : self . stop_training = True self . stopped_epoch = epoch_ix","title":"on_epoch_end()"},{"location":"api/torch_callbacks/#s4dd.torch_callbacks.HistoryLogger","text":"Bases: TorchCallback A callback that saves the training history in the end of every epoch. Source code in s4dd/torch_callbacks.py 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 class HistoryLogger ( TorchCallback ): \"\"\"A callback that saves the training history in the end of every epoch.\"\"\" def __init__ ( self , savedir : str ) -> None : \"\"\"Creates a `HistoryLogger` instance. Parameters ---------- savedir : str The directory to save the training history to. \"\"\" super () . __init__ () self . savedir = savedir os . makedirs ( self . savedir , exist_ok = True ) def on_epoch_end ( self , history : Dict [ str , List [ float ]], ** kwargs ) -> None : \"\"\"Saves the training history in the end of every epoch. Parameters ---------- history : Dict[str, List[float]] A dictionary containing the training history. The keys are the names of the metrics (`\"val_loss\"` and `\"train_loss\"`), and the values are lists of the metric values at each epoch. \"\"\" with open ( os . path . join ( self . savedir , \"history.json\" ), \"w\" ) as f : json . dump ( history , f , indent = 4 ) json . dump ( history , f , indent = 4 )","title":"HistoryLogger"},{"location":"api/torch_callbacks/#s4dd.torch_callbacks.HistoryLogger.__init__","text":"Creates a HistoryLogger instance. Parameters: Name Type Description Default savedir str The directory to save the training history to. required Source code in s4dd/torch_callbacks.py 220 221 222 223 224 225 226 227 228 229 230 def __init__ ( self , savedir : str ) -> None : \"\"\"Creates a `HistoryLogger` instance. Parameters ---------- savedir : str The directory to save the training history to. \"\"\" super () . __init__ () self . savedir = savedir os . makedirs ( self . savedir , exist_ok = True )","title":"__init__()"},{"location":"api/torch_callbacks/#s4dd.torch_callbacks.HistoryLogger.on_epoch_end","text":"Saves the training history in the end of every epoch. Parameters: Name Type Description Default history Dict [ str , List [ float ]] A dictionary containing the training history. The keys are the names of the metrics ( \"val_loss\" and \"train_loss\" ), and the values are lists of the metric values at each epoch. required Source code in s4dd/torch_callbacks.py 232 233 234 235 236 237 238 239 240 241 242 def on_epoch_end ( self , history : Dict [ str , List [ float ]], ** kwargs ) -> None : \"\"\"Saves the training history in the end of every epoch. Parameters ---------- history : Dict[str, List[float]] A dictionary containing the training history. The keys are the names of the metrics (`\"val_loss\"` and `\"train_loss\"`), and the values are lists of the metric values at each epoch. \"\"\" with open ( os . path . join ( self . savedir , \"history.json\" ), \"w\" ) as f : json . dump ( history , f , indent = 4 ) json . dump ( history , f , indent = 4 )","title":"on_epoch_end()"},{"location":"api/torch_callbacks/#s4dd.torch_callbacks.ModelCheckpoint","text":"Bases: TorchCallback A callback that saves the model in the end of every epoch. Source code in s4dd/torch_callbacks.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 class ModelCheckpoint ( TorchCallback ): \"\"\"A callback that saves the model in the end of every epoch.\"\"\" def __init__ ( self , save_fn : Callable [[ str ], None ], save_per_epoch : int , basedir : str , ) -> None : \"\"\"Creates a `ModelCheckpoint` instance that runs per a fixed number of epoch and at the end of training. Parameters ---------- save_fn : Callable[[str], None] A function that takes a directory and saves the model to that directory. save_per_epoch : int The number of epochs to wait between saves. basedir : str The base directory to save the model to. \"\"\" super () . __init__ () self . save_fn = save_fn self . save_per_epoch = save_per_epoch self . basedir = basedir def _save ( self , epoch_ix : int , ** kwargs ) -> None : savedir = os . path . join ( self . basedir , f \"epoch- { epoch_ix : 03d } \" ) os . makedirs ( savedir , exist_ok = True ) self . save_fn ( savedir ) def on_epoch_end ( self , epoch_ix : int , ** kwargs ) -> None : \"\"\"Saves the model in the end of every epoch. Parameters ---------- epoch_ix : int The index of the epoch that just ended. \"\"\" epoch_ix = epoch_ix + 1 # 1-indexed if epoch_ix % self . save_per_epoch == 0 : self . _save ( epoch_ix ) def on_train_end ( self , epoch_ix : int , ** kwargs ) -> None : \"\"\"Saves the model in the end of training. Parameters ---------- epoch_ix : int The index of the epoch that just ended. \"\"\" self . _save ( epoch_ix + 1 )","title":"ModelCheckpoint"},{"location":"api/torch_callbacks/#s4dd.torch_callbacks.ModelCheckpoint.__init__","text":"Creates a ModelCheckpoint instance that runs per a fixed number of epoch and at the end of training. Parameters: Name Type Description Default save_fn Callable [[ str ], None] A function that takes a directory and saves the model to that directory. required save_per_epoch int The number of epochs to wait between saves. required basedir str The base directory to save the model to. required Source code in s4dd/torch_callbacks.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def __init__ ( self , save_fn : Callable [[ str ], None ], save_per_epoch : int , basedir : str , ) -> None : \"\"\"Creates a `ModelCheckpoint` instance that runs per a fixed number of epoch and at the end of training. Parameters ---------- save_fn : Callable[[str], None] A function that takes a directory and saves the model to that directory. save_per_epoch : int The number of epochs to wait between saves. basedir : str The base directory to save the model to. \"\"\" super () . __init__ () self . save_fn = save_fn self . save_per_epoch = save_per_epoch self . basedir = basedir","title":"__init__()"},{"location":"api/torch_callbacks/#s4dd.torch_callbacks.ModelCheckpoint.on_epoch_end","text":"Saves the model in the end of every epoch. Parameters: Name Type Description Default epoch_ix int The index of the epoch that just ended. required Source code in s4dd/torch_callbacks.py 193 194 195 196 197 198 199 200 201 202 203 204 def on_epoch_end ( self , epoch_ix : int , ** kwargs ) -> None : \"\"\"Saves the model in the end of every epoch. Parameters ---------- epoch_ix : int The index of the epoch that just ended. \"\"\" epoch_ix = epoch_ix + 1 # 1-indexed if epoch_ix % self . save_per_epoch == 0 : self . _save ( epoch_ix )","title":"on_epoch_end()"},{"location":"api/torch_callbacks/#s4dd.torch_callbacks.ModelCheckpoint.on_train_end","text":"Saves the model in the end of training. Parameters: Name Type Description Default epoch_ix int The index of the epoch that just ended. required Source code in s4dd/torch_callbacks.py 206 207 208 209 210 211 212 213 214 def on_train_end ( self , epoch_ix : int , ** kwargs ) -> None : \"\"\"Saves the model in the end of training. Parameters ---------- epoch_ix : int The index of the epoch that just ended. \"\"\" self . _save ( epoch_ix + 1 )","title":"on_train_end()"},{"location":"api/torch_callbacks/#s4dd.torch_callbacks.TorchCallback","text":"Bases: ABC Base class for all Torch callbacks. Source code in s4dd/torch_callbacks.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 class TorchCallback ( ABC ): \"\"\"Base class for all Torch callbacks.\"\"\" def __init__ ( self ) -> None : \"\"\"Creates a TorchCallback. Sets the `stop_training` flag to `False`, which would be common attribute of all callbacks.\"\"\" super () . __init__ () self . stop_training = False def on_epoch_end ( self , epoch_ix , history , ** kwargs ): \"\"\"Called at the end of an epoch. Parameters ---------- epoch_ix : int The index of the epoch that just ended. history : Dict[str, List[float]] A dictionary containing the training history. The keys are the names of the metrics, and the values are lists of the metric values at each epoch. **kwargs Any additional keyword arguments. \"\"\" pass def on_train_end ( self , epoch_ix , history , ** kwargs ): \"\"\"Called at the end of training. Parameters ---------- epoch_ix : int The index of the epoch that just ended. history : Dict[str, List[float]] A dictionary containing the training history. The keys are the names of the metrics, and the values are lists of the metric values at each epoch. **kwargs Any additional keyword arguments. \"\"\" pass","title":"TorchCallback"},{"location":"api/torch_callbacks/#s4dd.torch_callbacks.TorchCallback.__init__","text":"Creates a TorchCallback. Sets the stop_training flag to False , which would be common attribute of all callbacks. Source code in s4dd/torch_callbacks.py 14 15 16 17 def __init__ ( self ) -> None : \"\"\"Creates a TorchCallback. Sets the `stop_training` flag to `False`, which would be common attribute of all callbacks.\"\"\" super () . __init__ () self . stop_training = False","title":"__init__()"},{"location":"api/torch_callbacks/#s4dd.torch_callbacks.TorchCallback.on_epoch_end","text":"Called at the end of an epoch. Parameters: Name Type Description Default epoch_ix int The index of the epoch that just ended. required history Dict [ str , List [ float ]] A dictionary containing the training history. The keys are the names of the metrics, and the values are lists of the metric values at each epoch. required **kwargs Any additional keyword arguments. {} Source code in s4dd/torch_callbacks.py 19 20 21 22 23 24 25 26 27 28 29 30 31 def on_epoch_end ( self , epoch_ix , history , ** kwargs ): \"\"\"Called at the end of an epoch. Parameters ---------- epoch_ix : int The index of the epoch that just ended. history : Dict[str, List[float]] A dictionary containing the training history. The keys are the names of the metrics, and the values are lists of the metric values at each epoch. **kwargs Any additional keyword arguments. \"\"\" pass","title":"on_epoch_end()"},{"location":"api/torch_callbacks/#s4dd.torch_callbacks.TorchCallback.on_train_end","text":"Called at the end of training. Parameters: Name Type Description Default epoch_ix int The index of the epoch that just ended. required history Dict [ str , List [ float ]] A dictionary containing the training history. The keys are the names of the metrics, and the values are lists of the metric values at each epoch. required **kwargs Any additional keyword arguments. {} Source code in s4dd/torch_callbacks.py 33 34 35 36 37 38 39 40 41 42 43 44 45 def on_train_end ( self , epoch_ix , history , ** kwargs ): \"\"\"Called at the end of training. Parameters ---------- epoch_ix : int The index of the epoch that just ended. history : Dict[str, List[float]] A dictionary containing the training history. The keys are the names of the metrics, and the values are lists of the metric values at each epoch. **kwargs Any additional keyword arguments. \"\"\" pass","title":"on_train_end()"}]}